{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668f1d1f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T23:26:27.799398Z",
     "iopub.status.busy": "2024-12-12T23:26:27.798714Z",
     "iopub.status.idle": "2024-12-12T23:26:47.011205Z",
     "shell.execute_reply": "2024-12-12T23:26:47.010503Z"
    },
    "papermill": {
     "duration": 19.220767,
     "end_time": "2024-12-12T23:26:47.013331",
     "exception": false,
     "start_time": "2024-12-12T23:26:27.792564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from setfit import SetFitModel\n",
    "import huggingface_hub\n",
    "from datasets import Dataset\n",
    "from transformers import DebertaForSequenceClassification, DebertaTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from joblib import load\n",
    "\n",
    "# can be 'setfit', 'deberta' 'tfidf', or 'knn'\n",
    "evaluation_type = 'setfit'\n",
    "\n",
    "# setfit_path = '/kaggle/input/automated-essay-scoring-setfit/transformers/default'\n",
    "setfit_path = '/kaggle/input/automated-essay-scoring-setfit-finetuned/transformers/default'\n",
    "deberta_path = '/kaggle/input/automated-essay-scoring-deberta/transformers/default'\n",
    "longformer_path = '/kaggle/input/allenailongformer-base-4096/transformers/default'\n",
    "allMiniLM_path = '/kaggle/input/all-minilm-l6-v2/transformers/default'\n",
    "# knn_path = '/kaggle/input/automated-essay-scoring-knn/scikitlearn/default'\n",
    "knn_path = '/kaggle/input/automated-essay-scoring-knn-all-features/scikitlearn/default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec3cae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T23:26:47.021894Z",
     "iopub.status.busy": "2024-12-12T23:26:47.021295Z",
     "iopub.status.idle": "2024-12-12T23:26:47.031845Z",
     "shell.execute_reply": "2024-12-12T23:26:47.030881Z"
    },
    "papermill": {
     "duration": 0.016681,
     "end_time": "2024-12-12T23:26:47.033720",
     "exception": false,
     "start_time": "2024-12-12T23:26:47.017039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SetFit Path: /kaggle/input/automated-essay-scoring-setfit-finetuned/transformers/default/3\n",
      "DeBERTa Path: /kaggle/input/automated-essay-scoring-deberta/transformers/default/3\n",
      "Longformer Path: /kaggle/input/allenailongformer-base-4096/transformers/default/1\n",
      "all-minilm-l6-v2 Path: /kaggle/input/all-minilm-l6-v2/transformers/default/1\n",
      "knn Path: /kaggle/input/automated-essay-scoring-knn-all-features/scikitlearn/default/1\n"
     ]
    }
   ],
   "source": [
    "def get_latest_version_path(base_path):\n",
    "    # List all directories in the base path\n",
    "    versions = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    \n",
    "    # Sort directories to find the latest version based on the highest number\n",
    "    latest_version = sorted(versions, key=lambda x: int(x), reverse=True)[0]\n",
    "    return os.path.join(base_path, latest_version)\n",
    "\n",
    "# Get full paths with the latest version number appended\n",
    "setfit_path = get_latest_version_path(setfit_path)\n",
    "deberta_path = get_latest_version_path(deberta_path)\n",
    "longformer_path = get_latest_version_path(longformer_path)\n",
    "allMiniLM_path = get_latest_version_path(allMiniLM_path)\n",
    "knn_path = get_latest_version_path(knn_path)\n",
    "\n",
    "# Print the paths to verify\n",
    "print(\"SetFit Path:\", setfit_path)\n",
    "print(\"DeBERTa Path:\", deberta_path)\n",
    "print(\"Longformer Path:\", longformer_path)\n",
    "print(\"all-minilm-l6-v2 Path:\", allMiniLM_path)\n",
    "print(\"knn Path:\", knn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb52a1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T23:26:47.042862Z",
     "iopub.status.busy": "2024-12-12T23:26:47.042302Z",
     "iopub.status.idle": "2024-12-12T23:26:48.177028Z",
     "shell.execute_reply": "2024-12-12T23:26:48.176060Z"
    },
    "papermill": {
     "duration": 1.141054,
     "end_time": "2024-12-12T23:26:48.179318",
     "exception": false,
     "start_time": "2024-12-12T23:26:47.038264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "# Set the device to GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ef810",
   "metadata": {
    "papermill": {
     "duration": 0.00377,
     "end_time": "2024-12-12T23:26:48.187190",
     "exception": false,
     "start_time": "2024-12-12T23:26:48.183420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SetFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ee48d03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T23:26:48.196439Z",
     "iopub.status.busy": "2024-12-12T23:26:48.195849Z",
     "iopub.status.idle": "2024-12-12T23:27:25.925669Z",
     "shell.execute_reply": "2024-12-12T23:27:25.924626Z"
    },
    "papermill": {
     "duration": 37.736265,
     "end_time": "2024-12-12T23:27:25.927501",
     "exception": false,
     "start_time": "2024-12-12T23:26:48.191236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/ipywidgets/widgets/widget.py:503: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  self.comm = Comm(**args)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e93d3d5dda4c0da3077c90dc09049f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
     ]
    }
   ],
   "source": [
    "if (evaluation_type == 'setfit'):\n",
    "\n",
    "    model = SetFitModel.from_pretrained(setfit_path)\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "\n",
    "    predictions = model(test_dataset['full_text'])\n",
    "\n",
    "    predictions = [int(pred) for pred in predictions]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fff080",
   "metadata": {
    "papermill": {
     "duration": 0.004013,
     "end_time": "2024-12-12T23:27:25.936260",
     "exception": false,
     "start_time": "2024-12-12T23:27:25.932247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196160e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T23:27:25.945341Z",
     "iopub.status.busy": "2024-12-12T23:27:25.945077Z",
     "iopub.status.idle": "2024-12-12T23:27:25.952433Z",
     "shell.execute_reply": "2024-12-12T23:27:25.951628Z"
    },
    "papermill": {
     "duration": 0.013601,
     "end_time": "2024-12-12T23:27:25.954000",
     "exception": false,
     "start_time": "2024-12-12T23:27:25.940399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to convert continuous scores into grades (1 to 6)\n",
    "def bin_to_grades(scores):\n",
    "    # Use np.digitize to categorize scores into grades 1 through 6\n",
    "    # Bins define the cutoff points for each grade, e.g., 0.5 to 1.5 is grade 1, etc.\n",
    "    bins = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]  # The edges for binning\n",
    "    grades = np.digitize(scores, bins, right=True)\n",
    "    return grades\n",
    "\n",
    "if (evaluation_type == 'deberta'):\n",
    "\n",
    "    model = DebertaForSequenceClassification.from_pretrained(deberta_path)\n",
    "    tokenizer = DebertaTokenizer.from_pretrained(deberta_path)\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predicted_scores = []\n",
    "\n",
    "    # Tokenize the test dataset\n",
    "    print(\"Tokenizing test dataset...\")\n",
    "    test_dataset = test_dataset.map(lambda x: tokenizer(x['full_text'], truncation=True, padding='max_length', max_length=256), batched=True)\n",
    "    \n",
    "    # Set the format for PyTorch (only 'input_ids' and 'attention_mask' are required for inference)\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    print(\"Test dataset tokenized and formatted successfully.\")\n",
    "\n",
    "    # Ensure predicted_scores is initialized\n",
    "    predicted_scores = []\n",
    "    \n",
    "    # Run predictions in batches\n",
    "    print(\"Generating predictions on the test dataset...\")\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in torch.utils.data.DataLoader(test_dataset, batch_size=20):  # Adjust batch size if needed\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "    \n",
    "            # Forward pass to get logits\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.squeeze(-1)  # For regression, logits should be a single value per input\n",
    "    \n",
    "            # Collect predictions\n",
    "            predicted_scores.extend(logits.cpu().numpy())\n",
    "    \n",
    "    # Convert the continuous scores to grades\n",
    "    predictions = bin_to_grades(predicted_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3a045",
   "metadata": {
    "papermill": {
     "duration": 0.00333,
     "end_time": "2024-12-12T23:27:25.960933",
     "exception": false,
     "start_time": "2024-12-12T23:27:25.957603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57300f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T23:27:25.969331Z",
     "iopub.status.busy": "2024-12-12T23:27:25.969091Z",
     "iopub.status.idle": "2024-12-12T23:30:15.439102Z",
     "shell.execute_reply": "2024-12-12T23:30:15.438359Z"
    },
    "papermill": {
     "duration": 169.477062,
     "end_time": "2024-12-12T23:30:15.441441",
     "exception": false,
     "start_time": "2024-12-12T23:27:25.964379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/google/cloud/bigquery/_http.py:18: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2309: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(parent)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.iam')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import shutil\n",
    "# import language_tool_python\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import syllapy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from lexical_diversity import lex_div as ld\n",
    "import pickle\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load spaCy model\n",
    "with open(\"/kaggle/input/spacy-en-core-web-sm/spacy_en_core_web_sm.pkl\", \"rb\") as file:\n",
    "    nlp = pickle.load(file)\n",
    "\n",
    "# Load NLTK\n",
    "paths = [(\"/kaggle/input/spacy-en-core-web-sm/punkt_tab\", \"/root/nltk_data/tokenizers\"), (\"/kaggle/input/spacy-en-core-web-sm/averaged_perceptron_tagger_eng\", \"/root/nltk_data/taggers\")]\n",
    "for path in paths:\n",
    "    source_folder_path = path[0]\n",
    "    target_folder_path = path[1]\n",
    "    \n",
    "    # Ensure the target directory exists\n",
    "    os.makedirs(target_folder_path, exist_ok=True)\n",
    "    \n",
    "    # Copy the folder to the target location\n",
    "    shutil.copytree(source_folder_path, target_folder_path, dirs_exist_ok=True)\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# pickle_file_path = \"spacy_en_core_web_sm.pkl\"\n",
    "# with open(pickle_file_path, \"wb\") as file:\n",
    "#     pickle.dump(nlp, file)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = [text for text in train_dataset['full_text']] + [text for text in test_dataset['full_text']]\n",
    "\n",
    "# Vocabulary Maturity using frequency as a proxy\n",
    "from nltk.corpus import brown  # Brown corpus for word frequencies\n",
    "word_frequencies = Counter(brown.words())  # Get frequencies from a standard corpus\n",
    "\n",
    "# Load Longformer model and tokenizer, supporting sequences up to 4096 tokens\n",
    "longformer_model = LongformerModel.from_pretrained(longformer_path)\n",
    "tokenizer = LongformerTokenizer.from_pretrained(longformer_path)\n",
    "\n",
    "# Move the model to the selected device\n",
    "longformer_model.to(device)\n",
    "\n",
    "embedding_model = SentenceTransformer(allMiniLM_path)\n",
    "\n",
    "# tool = language_tool_python.LanguageTool('en-US', remote_server='http://0.0.0.0:8081')\n",
    "spell = SpellChecker()\n",
    "\n",
    "# preprocessing\n",
    "def extract_linguistic_lexical_features(example):\n",
    "    # Sentence Count\n",
    "    sentences = sent_tokenize(example['full_text'])\n",
    "    sentence_count = len(sentences)\n",
    "    \n",
    "    # Average Sentence Length\n",
    "    avg_sentence_length = len(word_tokenize(example['full_text'])) / sentence_count if sentence_count > 0 else 0\n",
    "\n",
    "    # POS Tagging\n",
    "    words = word_tokenize(example['full_text'])\n",
    "    pos_tags = pos_tag(words)\n",
    "    pos_counts = Counter(tag for _, tag in pos_tags)\n",
    "\n",
    "    # Count specific POS tags\n",
    "    pos_noun_count = sum(pos_counts[tag] for tag in ['NN', 'NNS', 'NNP', 'NNPS'])\n",
    "    pos_verb_count = sum(pos_counts[tag] for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "    pos_adj_count = sum(pos_counts[tag] for tag in ['JJ', 'JJR', 'JJS'])\n",
    "    pos_adv_count = sum(pos_counts[tag] for tag in ['RB', 'RBR', 'RBS'])\n",
    "\n",
    "    example['sentence_count'] = sentence_count\n",
    "    example['average_sentence_length'] = avg_sentence_length\n",
    "    example['pos_noun_count'] = pos_noun_count\n",
    "    example['pos_verb_count'] = pos_verb_count\n",
    "    example['pos_adj_count'] = pos_adj_count\n",
    "    example['pos_adv_count'] = pos_adv_count\n",
    "    \n",
    "    return example\n",
    "\n",
    "def remove_punctuation_except_apostrophe(text):\n",
    "    # Define the punctuation to be removed (exclude apostrophes)\n",
    "    punctuation_to_remove = string.punctuation.replace(\"'\", \"\")\n",
    "    return text.translate(str.maketrans('', '', punctuation_to_remove))\n",
    "\n",
    "def extract_error_based_features(example):\n",
    "    # Grammar Error Count using LanguageTool\n",
    "    clean_text = remove_punctuation_except_apostrophe(example)\n",
    "    words = clean_text.split()  # Simple word splitting\n",
    "    misspelled = spell_checker.unknown(words)  # Identify misspelled words\n",
    "    # Count how often each misspelled word appears\n",
    "    grammar_error_count = Counter([word for word in words if word in misspelled])\n",
    "\n",
    "    # grammar_matches = tool.check(example['full_text'])\n",
    "    # grammar_error_count = len(grammar_matches)\n",
    "    \n",
    "    # Syntactic Complexity Calculation with Spacy\n",
    "    doc = nlp(example['full_text'])\n",
    "    sentence_depths = [len([token for token in sentence if token.dep_ != 'punct']) for sentence in doc.sents]\n",
    "    syntactic_complexity = sum(sentence_depths) / len(sentence_depths) if sentence_depths else 0\n",
    "\n",
    "    # Spelling Mistake Count using TextBlob\n",
    "    blob = TextBlob(example['full_text'])\n",
    "    spelling_mistake_count = sum(1 for word in blob.words if word.correct() != word)\n",
    "\n",
    "    # Error Density \n",
    "    word_count = len(blob.words)\n",
    "    error_density = (grammar_error_count + spelling_mistake_count) / word_count if word_count > 0 else 0\n",
    "\n",
    "    example['grammar_error_count'] = grammar_error_count\n",
    "    example['syntactic_complexity'] = syntactic_complexity\n",
    "    example['spelling_mistake_count'] = spelling_mistake_count\n",
    "    example['error_density'] = error_density\n",
    "    \n",
    "    return example\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "TOP_N_KEYWORDS = 100  \n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words), max_features=TOP_N_KEYWORDS)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Reduce dimensionality of the TF-IDF matrix to 50 dimensions\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "tfidf_reduced = svd.fit_transform(tfidf_matrix)\n",
    "tfidf_keywords_vectors = tfidf_reduced.tolist()\n",
    "\n",
    "# Tokenize and prepare corpus for LDA\n",
    "tokenized_corpus = [[word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words] for doc in corpus]\n",
    "dictionary = Dictionary(tokenized_corpus)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_corpus]\n",
    "\n",
    "NUM_TOPICS = 15 \n",
    "lda_model = LdaModel(bow_corpus, num_topics=NUM_TOPICS, id2word=dictionary, passes=10)\n",
    "\n",
    "# Function to generate topic coherence vector for each essay\n",
    "def lda_topic_vector(text):\n",
    "    bow = dictionary.doc2bow([word for word in word_tokenize(text.lower()) if word.isalpha()])\n",
    "    topic_distribution = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
    "    return [prob for _, prob in topic_distribution]\n",
    "\n",
    "# Define high-frequency keywords based on TF-IDF\n",
    "top_keywords = set(vectorizer.get_feature_names_out())\n",
    "\n",
    "def keyword_coverage_ratio(text):\n",
    "    words_in_text = set(word_tokenize(text.lower()))\n",
    "    coverage_ratio = len(top_keywords & words_in_text) / len(top_keywords)\n",
    "    return coverage_ratio\n",
    "\n",
    "def extract_semantic_features(example):\n",
    "    text = example['full_text']\n",
    "    response = vectorizer.transform([text])\n",
    "    tfidf_vector = svd.transform(response)  # 50-dimensional vector\n",
    "\n",
    "    lda_vector = lda_topic_vector(text)\n",
    "\n",
    "    coverage_ratio = keyword_coverage_ratio(text)\n",
    "    \n",
    "    example['tfidf_keywords_vector'] = tfidf_vector.tolist()[0]\n",
    "    example['lda_topic_vector'] = lda_vector\n",
    "    example['keyword_coverage'] = coverage_ratio\n",
    "    \n",
    "    return example\n",
    "\n",
    "def is_long_word(word):\n",
    "    \"\"\"Checks if a word has 3 or more syllables.\"\"\"\n",
    "    return syllapy.count(word) >= 3\n",
    "\n",
    "def is_imagery_word(word):\n",
    "    \"\"\"Checks if a word is an imagery word, based on WordNet's synsets.\"\"\"\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return False\n",
    "    imagery_tags = ['noun.artifact', 'noun.object', 'noun.plant', 'noun.animal', 'noun.body']\n",
    "    return any(tag in str(synset.lexname()) for synset in synsets for tag in imagery_tags)\n",
    "\n",
    "def extract_stylistic_features(example):\n",
    "    words = [word for word in word_tokenize(example['full_text'].lower()) if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    pos_tags = pos_tag(words)\n",
    "    pronoun_count = sum(1 for _, tag in pos_tags if tag in ['PRP', 'PRP$', 'WP', 'WP$'])\n",
    "    example['pronoun_usage'] = pronoun_count / len(words) if words else 0\n",
    "\n",
    "    long_words = [word for word in words if is_long_word(word)]\n",
    "    imagery_words = [word for word in words if is_imagery_word(word)]\n",
    "\n",
    "    unique_words = set(words)\n",
    "    example['unique_word_proportion'] = len(unique_words) / len(words) if words else 0\n",
    "\n",
    "    example['long_word_proportion'] = len(long_words) / len(words) if words else 0\n",
    "    example['imagery_word_proportion'] = len(imagery_words) / len(words) if words else 0\n",
    "\n",
    "    blob = TextBlob(example['full_text'])\n",
    "    example['positive_sentiment_score'] = blob.sentiment.polarity if blob.sentiment.polarity > 0 else 0\n",
    "    example['negative_sentiment_score'] = -blob.sentiment.polarity if blob.sentiment.polarity < 0 else 0\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "def is_visual_word(word):\n",
    "    \"\"\"Checks if a word evokes visual imagery, using WordNet synsets.\"\"\"\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return False\n",
    "    \n",
    "    visual_tags = ['noun.artifact', 'noun.object', 'noun.plant', 'noun.animal', 'noun.body', 'adj.all']\n",
    "    return any(tag in str(synset.lexname()) for synset in synsets for tag in visual_tags)\n",
    "\n",
    "def extract_visual_descriptive_features(example):\n",
    "    words = [word for word in word_tokenize(example['full_text'].lower()) if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    visual_words = [word for word in words if is_visual_word(word)]\n",
    "    unique_visual_words = set(visual_words)\n",
    "\n",
    "    example['visual_word_proportion'] = len(visual_words) / len(words) if words else 0\n",
    "    example['unique_visual_word_proportion'] = len(unique_visual_words) / len(words) if words else 0\n",
    "    example['average_imagery_score'] = len(visual_words) / len(unique_visual_words) if unique_visual_words else 0\n",
    "\n",
    "    return example\n",
    "\n",
    "def extract_cohesion_features(example):\n",
    "    \n",
    "    # Discourse Coherence with Dependency Parsing\n",
    "    doc = nlp(example['full_text'])\n",
    "    discourse_marker_count = 0\n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"mark\", \"cc\", \"advmod\"}:  \n",
    "            discourse_marker_count += 1\n",
    "\n",
    "    words = [token.text for token in doc if token.is_alpha]\n",
    "    example['discourse_marker_count'] = discourse_marker_count / len(words) if words else 0\n",
    "\n",
    "    # Neural Coherence Score\n",
    "    sentences = sent_tokenize(example['full_text'])\n",
    "    if len(sentences) > 1:\n",
    "        sentence_embeddings = embedding_model.encode(sentences)\n",
    "        similarities = [\n",
    "            np.dot(sentence_embeddings[i], sentence_embeddings[i + 1]) / \n",
    "            (np.linalg.norm(sentence_embeddings[i]) * np.linalg.norm(sentence_embeddings[i + 1]))\n",
    "            for i in range(len(sentence_embeddings) - 1)\n",
    "        ]\n",
    "        example['neural_coherence_score'] = np.mean(similarities) if similarities else 0\n",
    "    else:\n",
    "        example['neural_coherence_score'] = 0  \n",
    "\n",
    "    return example\n",
    "\n",
    "# Function to get Longformer sentence embeddings\n",
    "def get_longformer_embedding(text):\n",
    "    # Tokenize the text and move input tensors to the same device as the model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = longformer_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Move to CPU for numpy compatibility\n",
    "    return embeddings\n",
    "\n",
    "# Function to calculate coherence score using cosine similarity between sentence embeddings\n",
    "def calculate_coherence_score(text):\n",
    "    sentences = text.split('.')\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Get Longformer embeddings for each sentence\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip():  # Avoid empty sentences\n",
    "            embedding = get_longformer_embedding(sentence.strip())\n",
    "            sentence_embeddings.append(embedding)\n",
    "\n",
    "    # Calculate pairwise cosine similarities between consecutive sentence embeddings\n",
    "    coherence_scores = [\n",
    "        cosine_similarity([sentence_embeddings[i]], [sentence_embeddings[i + 1]])[0][0]\n",
    "        for i in range(len(sentence_embeddings) - 1)\n",
    "    ]\n",
    "\n",
    "    return np.mean(coherence_scores) if coherence_scores else 0\n",
    "\n",
    "# Function to extract neural network-based features\n",
    "def extract_neural_features(example):\n",
    "    essay_embedding = get_longformer_embedding(example['full_text'])\n",
    "    example['longformer_sentence_embedding'] = essay_embedding\n",
    "\n",
    "    example['longformer_coherence_score'] = calculate_coherence_score(example['full_text'])\n",
    "\n",
    "    return example\n",
    "\n",
    "def calculate_frequency_score(words):\n",
    "    \"\"\"Calculates average frequency of words in a standard corpus, with lower frequencies indicating more sophisticated vocabulary.\"\"\"\n",
    "    frequencies = [word_frequencies[word.lower()] for word in words if word.lower() in word_frequencies]\n",
    "    return sum(frequencies) / len(frequencies) if frequencies else 0\n",
    "\n",
    "def extract_vocabulary_features(example):\n",
    "    # Tokenize and filter words\n",
    "    words = [word for word in word_tokenize(example['full_text'].lower()) if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    # Type-Token Ratio (TTR) via spaCy\n",
    "    doc = nlp(\" \".join(words))\n",
    "    unique_words = set(token.text for token in doc)\n",
    "    example['type_token_ratio'] = len(unique_words) / len(words) if words else 0\n",
    "\n",
    "    # Lexical Diversity (spacy-based diversity metric)\n",
    "    example['lexical_diversity'] = doc._.mtld if hasattr(doc._, 'mtld') else ld.mtld(words)  # Ensure spaCy extension or fallback\n",
    "    \n",
    "    # Vocabulary Maturity (using word frequency as proxy for rarity/sophistication)\n",
    "    example['vocabulary_maturity'] = calculate_frequency_score(words)\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "def apply_all_features(dataset):\n",
    "    features = []\n",
    "    \n",
    "    for example in dataset:\n",
    "        example = extract_linguistic_lexical_features(example)\n",
    "        example = extract_error_based_features(example)\n",
    "        example = extract_semantic_features(example)\n",
    "        example = extract_stylistic_features(example)\n",
    "        example = extract_visual_descriptive_features(example)\n",
    "        example = extract_cohesion_features(example)\n",
    "        example = extract_neural_features(example)\n",
    "        example = extract_vocabulary_features(example)\n",
    "        \n",
    "        features.append(example)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589470c",
   "metadata": {
    "papermill": {
     "duration": 0.003616,
     "end_time": "2024-12-12T23:30:15.449459",
     "exception": false,
     "start_time": "2024-12-12T23:30:15.445843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "413b5d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T23:30:15.457836Z",
     "iopub.status.busy": "2024-12-12T23:30:15.457537Z",
     "iopub.status.idle": "2024-12-12T23:30:15.461904Z",
     "shell.execute_reply": "2024-12-12T23:30:15.461191Z"
    },
    "papermill": {
     "duration": 0.010589,
     "end_time": "2024-12-12T23:30:15.463616",
     "exception": false,
     "start_time": "2024-12-12T23:30:15.453027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (evaluation_type == 'knn'):\n",
    "    # train_data_features = apply_all_features(train_dataset)\n",
    "    test_data_features = apply_all_features(test_dataset)\n",
    "\n",
    "    \n",
    "    # load model and do predictions\n",
    "    model = load(f\"{knn_path}/model.joblib\")\n",
    "    predictions = model.predict(test_data_features)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803c006",
   "metadata": {
    "papermill": {
     "duration": 0.003495,
     "end_time": "2024-12-12T23:30:15.470736",
     "exception": false,
     "start_time": "2024-12-12T23:30:15.467241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "772436ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T23:30:15.479553Z",
     "iopub.status.busy": "2024-12-12T23:30:15.478900Z",
     "iopub.status.idle": "2024-12-12T23:30:15.485440Z",
     "shell.execute_reply": "2024-12-12T23:30:15.484731Z"
    },
    "papermill": {
     "duration": 0.012518,
     "end_time": "2024-12-12T23:30:15.487056",
     "exception": false,
     "start_time": "2024-12-12T23:30:15.474538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (evaluation_type == 'tfidf'):\n",
    "\n",
    "    # Define the corpus from the dataset\n",
    "    train_corpus = [text for text in train_dataset['full_text']]\n",
    "    test_corpus = [text for text in test_dataset['full_text']]\n",
    "    \n",
    "    # Define the number of top keywords for TF-IDF and the number of components for SVD\n",
    "    CONFIGURATIONS = [(500, 250)]\n",
    "    \n",
    "    # Prepare a dictionary to hold datasets for each configuration of TOP_N_KEYWORDS and N_COMPONENTS\n",
    "    datasets = {}\n",
    "    \n",
    "    for TOP_N_KEYWORDS, N_COMPONENTS in CONFIGURATIONS:\n",
    "        # TF-IDF Vectorizer\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=TOP_N_KEYWORDS)\n",
    "        \n",
    "        # Fit and transform on train data, transform on test\n",
    "        tfidf_train = vectorizer.fit_transform(train_corpus)\n",
    "        tfidf_test = vectorizer.transform(test_corpus)\n",
    "    \n",
    "        if N_COMPONENTS is None:\n",
    "            # No dimensionality reduction, use the original TF-IDF vectors (convert to dense)\n",
    "            datasets[f'X_train_{TOP_N_KEYWORDS}'] = tfidf_train.toarray()\n",
    "            datasets[f'X_test_{TOP_N_KEYWORDS}'] = tfidf_test.toarray()\n",
    "        else:\n",
    "            # Reduce dimensionality of the TF-IDF matrix\n",
    "            svd = TruncatedSVD(n_components=N_COMPONENTS, random_state=42)\n",
    "            tfidf_train_reduced = svd.fit_transform(tfidf_train)\n",
    "            tfidf_test_reduced = svd.transform(tfidf_test)\n",
    "    \n",
    "            # Assign the reduced TF-IDF vectors to specific variables\n",
    "            datasets[f'X_train_{TOP_N_KEYWORDS}_{N_COMPONENTS}'] = tfidf_train_reduced\n",
    "            datasets[f'X_test_{TOP_N_KEYWORDS}_{N_COMPONENTS}'] = tfidf_test_reduced\n",
    "    \n",
    "    \n",
    "    X_train_500_250 = datasets['X_train_500_250']\n",
    "    X_test_500_250 = datasets['X_test_500_250']\n",
    "\n",
    "    # load model and do predictions\n",
    "    model = load(f\"{knn_path}/model.joblib\")\n",
    "    predictions = model.predict(X_test_500_250)\n",
    "    print(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4872ee46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T23:30:15.495925Z",
     "iopub.status.busy": "2024-12-12T23:30:15.495471Z",
     "iopub.status.idle": "2024-12-12T23:30:15.502850Z",
     "shell.execute_reply": "2024-12-12T23:30:15.502018Z"
    },
    "papermill": {
     "duration": 0.013471,
     "end_time": "2024-12-12T23:30:15.504461",
     "exception": false,
     "start_time": "2024-12-12T23:30:15.490990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add predictions to the Dataset using `map`\n",
    "test_dataset_with_predictions = test_dataset.add_column(\"predictions\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbe8699c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T23:30:15.513018Z",
     "iopub.status.busy": "2024-12-12T23:30:15.512284Z",
     "iopub.status.idle": "2024-12-12T23:30:15.518221Z",
     "shell.execute_reply": "2024-12-12T23:30:15.517412Z"
    },
    "papermill": {
     "duration": 0.011773,
     "end_time": "2024-12-12T23:30:15.519856",
     "exception": false,
     "start_time": "2024-12-12T23:30:15.508083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['essay_id', 'full_text', 'predictions'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_with_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "658bda1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T23:30:15.528307Z",
     "iopub.status.busy": "2024-12-12T23:30:15.528066Z",
     "iopub.status.idle": "2024-12-12T23:30:15.542462Z",
     "shell.execute_reply": "2024-12-12T23:30:15.541519Z"
    },
    "papermill": {
     "duration": 0.02109,
     "end_time": "2024-12-12T23:30:15.544809",
     "exception": false,
     "start_time": "2024-12-12T23:30:15.523719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = test_dataset_with_predictions.to_pandas()\n",
    "\n",
    "# Select and rename the required columns\n",
    "submission = submission[['essay_id', 'predictions']].rename(columns={'predictions': 'score'})\n",
    "\n",
    "# Save as CSV\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 6239009,
     "sourceId": 10185019,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 149785,
     "modelInstanceId": 126822,
     "sourceId": 149402,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 154819,
     "modelInstanceId": 132016,
     "sourceId": 155373,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 155597,
     "modelInstanceId": 132819,
     "sourceId": 156292,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 158205,
     "modelInstanceId": 135483,
     "sourceId": 159354,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 190044,
     "modelInstanceId": 167712,
     "sourceId": 196661,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 181853,
     "modelInstanceId": 160504,
     "sourceId": 196672,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 233.69393,
   "end_time": "2024-12-12T23:30:19.135066",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T23:26:25.441136",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0daf8de3a3e04f4cb3db09451556214c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3b81d19b1a6c426ca7e7bc532a16aef6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e0f22ca4f55e4733a4a7287a3adc5d96",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dbbe4ee9e28542e7ae2ae67704195c55",
       "value": 1.0
      }
     },
     "61c91da4fbee4ca6a26229ebbcde6780": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "65bd635147114045be519393e24c96ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68e93d3d5dda4c0da3077c90dc09049f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c764df4fc64b45e78f48f0e91537c484",
        "IPY_MODEL_3b81d19b1a6c426ca7e7bc532a16aef6",
        "IPY_MODEL_d404752f25504b9f999d10f54716414a"
       ],
       "layout": "IPY_MODEL_717b64b12598481699751a22fa196f26"
      }
     },
     "717b64b12598481699751a22fa196f26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ae20b5570164592bc1b8ece3dfe1a62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c764df4fc64b45e78f48f0e91537c484": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_61c91da4fbee4ca6a26229ebbcde6780",
       "placeholder": "​",
       "style": "IPY_MODEL_0daf8de3a3e04f4cb3db09451556214c",
       "value": "Batches: 100%"
      }
     },
     "d404752f25504b9f999d10f54716414a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_65bd635147114045be519393e24c96ce",
       "placeholder": "​",
       "style": "IPY_MODEL_9ae20b5570164592bc1b8ece3dfe1a62",
       "value": " 1/1 [00:00&lt;00:00,  1.18it/s]"
      }
     },
     "dbbe4ee9e28542e7ae2ae67704195c55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e0f22ca4f55e4733a4a7287a3adc5d96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
