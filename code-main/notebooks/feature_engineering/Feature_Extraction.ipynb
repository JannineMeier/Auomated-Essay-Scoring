{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in to Hugging Face Hub and W&B...\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/leonkrug/.cache/huggingface/token\n",
      "Login successful\n",
      "Login successful.\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Hugging Face and Weights & Biases setup\n",
    "huggingface_username = 'HSLU-AICOMP-LearningAgencyLab'\n",
    "competition = 'learning-agency-lab-automated-essay-scoring-2'\n",
    "\n",
    "# Login to Hugging Face and W&B\n",
    "print(\"Logging in to Hugging Face Hub and W&B...\")\n",
    "huggingface_hub.login(token=os.getenv('HUGGINGFACE_TOKEN'))\n",
    "print(\"Login successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the entire dataset from Hugging Face...\n",
      "Dataset loaded successfully.\n",
      "Inspecting the dataset...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['essay_id', 'full_text', 'score', 'unique_mistakes', 'repeated_mistakes_count', 'max_repeated_mistake', 'word_count', 'flesch_reading_ease', 'flesch_kincaid_grade'],\n",
      "        num_rows: 13845\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['essay_id', 'full_text', 'score', 'unique_mistakes', 'repeated_mistakes_count', 'max_repeated_mistake', 'word_count', 'flesch_reading_ease', 'flesch_kincaid_grade'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: ['essay_id', 'full_text', 'score', 'unique_mistakes', 'repeated_mistakes_count', 'max_repeated_mistake', 'word_count', 'flesch_reading_ease', 'flesch_kincaid_grade'],\n",
      "        num_rows: 3462\n",
      "    })\n",
      "})\n",
      "{'essay_id': 'ea26dc4', 'full_text': 'I belive that they should change the elections to elections by popular vote for the president of the United States.\\n\\nThe electoral College consists of 530 electors, tey choose the president by a vote in congress or \"qualified\" citizens. A majoriy of 270 electoral votes are required to elect a president, therefore in my opinion a popula vote would have mor meaning, beacause it\\'s more people the vote would be chosen fr the mayority making the rest of the people satisfied. The number of citizens all togete versus the number of the electors at the Electoral College is superior; the people are te ones that follow the law might as well let them choose he president thats going to run the country and establish laws.\\n\\nThe electoral college system prevents us from voting for the presidet directly, instead they make us vote for a slate of electors, who in turn elect the pesident. For example if you were to live in Texas and wanted to vote for a president you\\'d have to vote for a slate of 34 elector of that political party that are pleged to him or her, on the off-chance that those electors won the statewide elections. The electors get picked at state conventions, the states party\\'s central committee and sometimes the presidential candidates themselves. Also electos can be anyone not holding public office.\\n\\nFurthermore the best argument against the electoral college is the disaster factor. Americans should consider themselves lucky that the 2000 disaster was he biggest crisis in elections that century since the system allows much worse. Consider the fact that the state legislatures are in a way responsible for piking electors and those elector could defy the will of the people back in 1960; what tells you that they stopped doing it?\\n\\nThe electoral college system has a lo of flaws and they can choose if they want o do what the peopl think is best or just take mater into their own hands and do as they plese.\\n\\nIn conclution its very clear that they should get rid of the electoral college and just let the peopl vote directly fo the president. That would cause less drama and would prevent fiascos in the elections.       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0\\xa0\\xa0\\xa0\\xa0     ', 'score': 3, 'unique_mistakes': 20.0, 'repeated_mistakes_count': 2.0, 'max_repeated_mistake': 2.0, 'word_count': 371.0, 'flesch_reading_ease': 61.5, 'flesch_kincaid_grade': 11.3}\n"
     ]
    }
   ],
   "source": [
    "# Load the entire dataset from Hugging Face\n",
    "print(\"Loading the entire dataset from Hugging Face...\")\n",
    "dataset = load_dataset(f\"{huggingface_username}/{competition}\")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Inspect the dataset\n",
    "print(\"Inspecting the dataset...\")\n",
    "print(dataset)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic and lexical feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'essay_id': 'ea26dc4', 'full_text': 'I belive that they should change the elections to elections by popular vote for the president of the United States.\\n\\nThe electoral College consists of 530 electors, tey choose the president by a vote in congress or \"qualified\" citizens. A majoriy of 270 electoral votes are required to elect a president, therefore in my opinion a popula vote would have mor meaning, beacause it\\'s more people the vote would be chosen fr the mayority making the rest of the people satisfied. The number of citizens all togete versus the number of the electors at the Electoral College is superior; the people are te ones that follow the law might as well let them choose he president thats going to run the country and establish laws.\\n\\nThe electoral college system prevents us from voting for the presidet directly, instead they make us vote for a slate of electors, who in turn elect the pesident. For example if you were to live in Texas and wanted to vote for a president you\\'d have to vote for a slate of 34 elector of that political party that are pleged to him or her, on the off-chance that those electors won the statewide elections. The electors get picked at state conventions, the states party\\'s central committee and sometimes the presidential candidates themselves. Also electos can be anyone not holding public office.\\n\\nFurthermore the best argument against the electoral college is the disaster factor. Americans should consider themselves lucky that the 2000 disaster was he biggest crisis in elections that century since the system allows much worse. Consider the fact that the state legislatures are in a way responsible for piking electors and those elector could defy the will of the people back in 1960; what tells you that they stopped doing it?\\n\\nThe electoral college system has a lo of flaws and they can choose if they want o do what the peopl think is best or just take mater into their own hands and do as they plese.\\n\\nIn conclution its very clear that they should get rid of the electoral college and just let the peopl vote directly fo the president. That would cause less drama and would prevent fiascos in the elections.       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0\\xa0\\xa0\\xa0\\xa0     ', 'score': 3, 'unique_mistakes': 20.0, 'repeated_mistakes_count': 2.0, 'max_repeated_mistake': 2.0, 'word_count': 371.0, 'flesch_reading_ease': 61.5, 'flesch_kincaid_grade': 11.3, 'sentence_count': 14, 'average_sentence_length': 28.5, 'pos_noun_count': 95, 'pos_verb_count': 65, 'pos_adj_count': 27, 'pos_adv_count': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/leonkrug/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/leonkrug/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def extract_linguistic_lexical_features(example):\n",
    "    # Sentence Count\n",
    "    sentences = sent_tokenize(example['full_text'])\n",
    "    sentence_count = len(sentences)\n",
    "    \n",
    "    # Average Sentence Length\n",
    "    avg_sentence_length = len(word_tokenize(example['full_text'])) / sentence_count if sentence_count > 0 else 0\n",
    "\n",
    "    # POS Tagging\n",
    "    words = word_tokenize(example['full_text'])\n",
    "    pos_tags = pos_tag(words)\n",
    "    pos_counts = Counter(tag for _, tag in pos_tags)\n",
    "\n",
    "    # Count specific POS tags\n",
    "    pos_noun_count = sum(pos_counts[tag] for tag in ['NN', 'NNS', 'NNP', 'NNPS'])\n",
    "    pos_verb_count = sum(pos_counts[tag] for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "    pos_adj_count = sum(pos_counts[tag] for tag in ['JJ', 'JJR', 'JJS'])\n",
    "    pos_adv_count = sum(pos_counts[tag] for tag in ['RB', 'RBR', 'RBS'])\n",
    "\n",
    "    example['sentence_count'] = sentence_count\n",
    "    example['average_sentence_length'] = avg_sentence_length\n",
    "    example['pos_noun_count'] = pos_noun_count\n",
    "    example['pos_verb_count'] = pos_verb_count\n",
    "    example['pos_adj_count'] = pos_adj_count\n",
    "    example['pos_adv_count'] = pos_adv_count\n",
    "    \n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(extract_linguistic_lexical_features)\n",
    "\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar and Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function extract_error_based_features at 0x7d3bfb918040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Parameter 'function'=<function extract_error_based_features at 0x7d3bfb918040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9794219b1d354a22814af8de94d59b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999a1263332d4ab3a6b3f881f2df70d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca06686459640808ad6e0e1c9d095a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'essay_id': 'ea26dc4', 'full_text': 'I belive that they should change the elections to elections by popular vote for the president of the United States.\\n\\nThe electoral College consists of 530 electors, tey choose the president by a vote in congress or \"qualified\" citizens. A majoriy of 270 electoral votes are required to elect a president, therefore in my opinion a popula vote would have mor meaning, beacause it\\'s more people the vote would be chosen fr the mayority making the rest of the people satisfied. The number of citizens all togete versus the number of the electors at the Electoral College is superior; the people are te ones that follow the law might as well let them choose he president thats going to run the country and establish laws.\\n\\nThe electoral college system prevents us from voting for the presidet directly, instead they make us vote for a slate of electors, who in turn elect the pesident. For example if you were to live in Texas and wanted to vote for a president you\\'d have to vote for a slate of 34 elector of that political party that are pleged to him or her, on the off-chance that those electors won the statewide elections. The electors get picked at state conventions, the states party\\'s central committee and sometimes the presidential candidates themselves. Also electos can be anyone not holding public office.\\n\\nFurthermore the best argument against the electoral college is the disaster factor. Americans should consider themselves lucky that the 2000 disaster was he biggest crisis in elections that century since the system allows much worse. Consider the fact that the state legislatures are in a way responsible for piking electors and those elector could defy the will of the people back in 1960; what tells you that they stopped doing it?\\n\\nThe electoral college system has a lo of flaws and they can choose if they want o do what the peopl think is best or just take mater into their own hands and do as they plese.\\n\\nIn conclution its very clear that they should get rid of the electoral college and just let the peopl vote directly fo the president. That would cause less drama and would prevent fiascos in the elections.       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0\\xa0\\xa0\\xa0\\xa0     ', 'score': 3, 'unique_mistakes': 20.0, 'repeated_mistakes_count': 2.0, 'max_repeated_mistake': 2.0, 'word_count': 371.0, 'flesch_reading_ease': 61.5, 'flesch_kincaid_grade': 11.3, 'sentence_count': 14, 'average_sentence_length': 28.5, 'pos_noun_count': 95, 'pos_verb_count': 65, 'pos_adj_count': 27, 'pos_adv_count': 15, 'grammar_error_count': 30, 'syntactic_complexity': 27.285714285714285, 'spelling_mistake_count': 21, 'error_density': 0.13636363636363635}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import language_tool_python\n",
    "from textblob import TextBlob\n",
    "\n",
    "# RUN python -m spacy download en_core_web_sm first to download the model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "def extract_error_based_features(example):\n",
    "    # Grammar Error Count using LanguageTool\n",
    "    grammar_matches = tool.check(example['full_text'])\n",
    "    grammar_error_count = len(grammar_matches)\n",
    "    \n",
    "    # Syntactic Complexity Calculation with Spacy\n",
    "    doc = nlp(example['full_text'])\n",
    "    sentence_depths = [len([token for token in sentence if token.dep_ != 'punct']) for sentence in doc.sents]\n",
    "    syntactic_complexity = sum(sentence_depths) / len(sentence_depths) if sentence_depths else 0\n",
    "\n",
    "    # Spelling Mistake Count using TextBlob\n",
    "    blob = TextBlob(example['full_text'])\n",
    "    spelling_mistake_count = sum(1 for word in blob.words if word.correct() != word)\n",
    "\n",
    "    # Error Density \n",
    "    word_count = len(blob.words)\n",
    "    error_density = (grammar_error_count + spelling_mistake_count) / word_count if word_count > 0 else 0\n",
    "\n",
    "    example['grammar_error_count'] = grammar_error_count\n",
    "    example['syntactic_complexity'] = syntactic_complexity\n",
    "    example['spelling_mistake_count'] = spelling_mistake_count\n",
    "    example['error_density'] = error_density\n",
    "    \n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(extract_error_based_features)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic and content-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206e21cfeee6430781012cc4c23328de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86d1758b8514a5585fd516c8767fde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d84ad5b4cd4d6aae166d0de17315f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'essay_id': 'ea26dc4', 'full_text': 'I belive that they should change the elections to elections by popular vote for the president of the United States.\\n\\nThe electoral College consists of 530 electors, tey choose the president by a vote in congress or \"qualified\" citizens. A majoriy of 270 electoral votes are required to elect a president, therefore in my opinion a popula vote would have mor meaning, beacause it\\'s more people the vote would be chosen fr the mayority making the rest of the people satisfied. The number of citizens all togete versus the number of the electors at the Electoral College is superior; the people are te ones that follow the law might as well let them choose he president thats going to run the country and establish laws.\\n\\nThe electoral college system prevents us from voting for the presidet directly, instead they make us vote for a slate of electors, who in turn elect the pesident. For example if you were to live in Texas and wanted to vote for a president you\\'d have to vote for a slate of 34 elector of that political party that are pleged to him or her, on the off-chance that those electors won the statewide elections. The electors get picked at state conventions, the states party\\'s central committee and sometimes the presidential candidates themselves. Also electos can be anyone not holding public office.\\n\\nFurthermore the best argument against the electoral college is the disaster factor. Americans should consider themselves lucky that the 2000 disaster was he biggest crisis in elections that century since the system allows much worse. Consider the fact that the state legislatures are in a way responsible for piking electors and those elector could defy the will of the people back in 1960; what tells you that they stopped doing it?\\n\\nThe electoral college system has a lo of flaws and they can choose if they want o do what the peopl think is best or just take mater into their own hands and do as they plese.\\n\\nIn conclution its very clear that they should get rid of the electoral college and just let the peopl vote directly fo the president. That would cause less drama and would prevent fiascos in the elections.       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0\\xa0\\xa0\\xa0\\xa0     ', 'score': 3, 'unique_mistakes': 20.0, 'repeated_mistakes_count': 2.0, 'max_repeated_mistake': 2.0, 'word_count': 371.0, 'flesch_reading_ease': 61.5, 'flesch_kincaid_grade': 11.3, 'sentence_count': 14, 'average_sentence_length': 28.5, 'pos_noun_count': 95, 'pos_verb_count': 65, 'pos_adj_count': 27, 'pos_adv_count': 15, 'grammar_error_count': 30, 'syntactic_complexity': 27.285714285714285, 'spelling_mistake_count': 21, 'error_density': 0.13636363636363635, 'tfidf_keywords_vector': [0.1977982064113758, 0.042789746378051934, 0.6732435383791084, -0.6178010730898644, 0.08586393617102417, -0.0689542823154062, 0.00959424838752973, -0.00433862013677394, 0.015521928894408812, 0.028556710631481227, 0.003757644518672762, 0.04918226803064273, -0.010506208564133262, 0.048212915728345004, -0.029402427110320102, -0.009567565474337441, 0.13541366258786106, -0.030701949879952065, 0.03407629912825338, -0.011728986640566869, 0.04731903765236058, 0.024278565233402327, -0.0202055220873499, -0.00344041726757616, -0.0059983107793590154, -0.04064541259318033, 0.016375954682992234, -0.01135368585284111, 0.03860618222275983, 0.025629004680896985, -0.008082578985205964, -0.04885045629851237, 0.028809704988296735, -0.02570286456190781, 0.006637146444963989, 0.035600884341260595, 0.025131795491315403, -0.05378137159965886, 0.04980907506456732, -0.04061273791969073, 0.02232726496054869, 0.0870364054535328, 0.11083698930537171, -0.009844311301486856, 0.021896946448464568, -0.013370645226224857, -0.04935768852153144, 0.055487858614111533, 0.04341156952912014, -0.04025193578871404], 'lda_topic_vector': [0.00037597768823616207, 0.00037597730988636613, 0.011616736650466919, 0.00037597701884806156, 0.00037597701884806156, 0.00037597701884806156, 0.9190030097961426, 0.06058431416749954, 0.00037597757182084024, 0.000375977746443823, 0.0003759779501706362, 0.0003759773971978575, 0.004660164937376976, 0.00037597757182084024, 0.00037597748450934887], 'keyword_coverage': 0.28}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = [text for text in dataset['train']['full_text']]\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "TOP_N_KEYWORDS = 100  \n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words), max_features=TOP_N_KEYWORDS)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Reduce dimensionality of the TF-IDF matrix to 50 dimensions\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "tfidf_reduced = svd.fit_transform(tfidf_matrix)\n",
    "tfidf_keywords_vectors = tfidf_reduced.tolist()\n",
    "\n",
    "# Tokenize and prepare corpus for LDA\n",
    "tokenized_corpus = [[word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words] for doc in corpus]\n",
    "dictionary = Dictionary(tokenized_corpus)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_corpus]\n",
    "\n",
    "NUM_TOPICS = 15 \n",
    "lda_model = LdaModel(bow_corpus, num_topics=NUM_TOPICS, id2word=dictionary, passes=10)\n",
    "\n",
    "# Function to generate topic coherence vector for each essay\n",
    "def lda_topic_vector(text):\n",
    "    bow = dictionary.doc2bow([word for word in word_tokenize(text.lower()) if word.isalpha()])\n",
    "    topic_distribution = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
    "    return [prob for _, prob in topic_distribution]\n",
    "\n",
    "# Define high-frequency keywords based on TF-IDF\n",
    "top_keywords = set(vectorizer.get_feature_names_out())\n",
    "\n",
    "def keyword_coverage_ratio(text):\n",
    "    words_in_text = set(word_tokenize(text.lower()))\n",
    "    coverage_ratio = len(top_keywords & words_in_text) / len(top_keywords)\n",
    "    return coverage_ratio\n",
    "\n",
    "def extract_semantic_features(example):\n",
    "    text = example['full_text']\n",
    "    response = vectorizer.transform([text])\n",
    "    tfidf_vector = svd.transform(response)  # 50-dimensional vector\n",
    "\n",
    "    lda_vector = lda_topic_vector(text)\n",
    "\n",
    "    coverage_ratio = keyword_coverage_ratio(text)\n",
    "    \n",
    "    example['tfidf_keywords_vector'] = tfidf_vector.tolist()[0]\n",
    "    example['lda_topic_vector'] = lda_vector\n",
    "    example['keyword_coverage'] = coverage_ratio\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Apply the feature extraction to the dataset\n",
    "dataset = dataset.map(extract_semantic_features)\n",
    "\n",
    "# Check results\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stylistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b853b102f8f04806bce40a5e4b00fc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06178b6d833e467dba87361a6327313d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de5de87d8b74bf19122c7a1a0390cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'essay_id': 'ea26dc4', 'full_text': 'I belive that they should change the elections to elections by popular vote for the president of the United States.\\n\\nThe electoral College consists of 530 electors, tey choose the president by a vote in congress or \"qualified\" citizens. A majoriy of 270 electoral votes are required to elect a president, therefore in my opinion a popula vote would have mor meaning, beacause it\\'s more people the vote would be chosen fr the mayority making the rest of the people satisfied. The number of citizens all togete versus the number of the electors at the Electoral College is superior; the people are te ones that follow the law might as well let them choose he president thats going to run the country and establish laws.\\n\\nThe electoral college system prevents us from voting for the presidet directly, instead they make us vote for a slate of electors, who in turn elect the pesident. For example if you were to live in Texas and wanted to vote for a president you\\'d have to vote for a slate of 34 elector of that political party that are pleged to him or her, on the off-chance that those electors won the statewide elections. The electors get picked at state conventions, the states party\\'s central committee and sometimes the presidential candidates themselves. Also electos can be anyone not holding public office.\\n\\nFurthermore the best argument against the electoral college is the disaster factor. Americans should consider themselves lucky that the 2000 disaster was he biggest crisis in elections that century since the system allows much worse. Consider the fact that the state legislatures are in a way responsible for piking electors and those elector could defy the will of the people back in 1960; what tells you that they stopped doing it?\\n\\nThe electoral college system has a lo of flaws and they can choose if they want o do what the peopl think is best or just take mater into their own hands and do as they plese.\\n\\nIn conclution its very clear that they should get rid of the electoral college and just let the peopl vote directly fo the president. That would cause less drama and would prevent fiascos in the elections.       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0\\xa0\\xa0\\xa0\\xa0     ', 'score': 3, 'unique_mistakes': 20.0, 'repeated_mistakes_count': 2.0, 'max_repeated_mistake': 2.0, 'word_count': 371.0, 'flesch_reading_ease': 61.5, 'flesch_kincaid_grade': 11.3, 'sentence_count': 14, 'average_sentence_length': 28.5, 'pos_noun_count': 95, 'pos_verb_count': 65, 'pos_adj_count': 27, 'pos_adv_count': 15, 'grammar_error_count': 30, 'syntactic_complexity': 27.285714285714285, 'spelling_mistake_count': 21, 'error_density': 0.13636363636363635, 'tfidf_keywords_vector': [0.1977982064113758, 0.042789746378051934, 0.6732435383791084, -0.6178010730898644, 0.08586393617102417, -0.0689542823154062, 0.00959424838752973, -0.00433862013677394, 0.015521928894408812, 0.028556710631481227, 0.003757644518672762, 0.04918226803064273, -0.010506208564133262, 0.048212915728345004, -0.029402427110320102, -0.009567565474337441, 0.13541366258786106, -0.030701949879952065, 0.03407629912825338, -0.011728986640566869, 0.04731903765236058, 0.024278565233402327, -0.0202055220873499, -0.00344041726757616, -0.0059983107793590154, -0.04064541259318033, 0.016375954682992234, -0.01135368585284111, 0.03860618222275983, 0.025629004680896985, -0.008082578985205964, -0.04885045629851237, 0.028809704988296735, -0.02570286456190781, 0.006637146444963989, 0.035600884341260595, 0.025131795491315403, -0.05378137159965886, 0.04980907506456732, -0.04061273791969073, 0.02232726496054869, 0.0870364054535328, 0.11083698930537171, -0.009844311301486856, 0.021896946448464568, -0.013370645226224857, -0.04935768852153144, 0.055487858614111533, 0.04341156952912014, -0.04025193578871404], 'lda_topic_vector': [0.00037597768823616207, 0.00037597730988636613, 0.011616736650466919, 0.00037597701884806156, 0.00037597701884806156, 0.00037597701884806156, 0.9190030097961426, 0.06058431416749954, 0.00037597757182084024, 0.000375977746443823, 0.0003759779501706362, 0.0003759773971978575, 0.004660164937376976, 0.00037597757182084024, 0.00037597748450934887], 'keyword_coverage': 0.28, 'pronoun_usage': 0.010638297872340425, 'unique_word_proportion': 0.6914893617021277, 'long_word_proportion': 0.32978723404255317, 'imagery_word_proportion': 0.13829787234042554, 'positive_sentiment_score': 0.3301443001443001, 'negative_sentiment_score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from textblob import TextBlob\n",
    "import syllapy\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def is_long_word(word):\n",
    "    \"\"\"Checks if a word has 3 or more syllables.\"\"\"\n",
    "    return syllapy.count(word) >= 3\n",
    "\n",
    "def is_imagery_word(word):\n",
    "    \"\"\"Checks if a word is an imagery word, based on WordNet's synsets.\"\"\"\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return False\n",
    "    imagery_tags = ['noun.artifact', 'noun.object', 'noun.plant', 'noun.animal', 'noun.body']\n",
    "    return any(tag in str(synset.lexname()) for synset in synsets for tag in imagery_tags)\n",
    "\n",
    "def extract_stylistic_features(example):\n",
    "    words = [word for word in word_tokenize(example['full_text'].lower()) if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    pos_tags = pos_tag(words)\n",
    "    pronoun_count = sum(1 for _, tag in pos_tags if tag in ['PRP', 'PRP$', 'WP', 'WP$'])\n",
    "    example['pronoun_usage'] = pronoun_count / len(words) if words else 0\n",
    "\n",
    "    long_words = [word for word in words if is_long_word(word)]\n",
    "    imagery_words = [word for word in words if is_imagery_word(word)]\n",
    "\n",
    "    unique_words = set(words)\n",
    "    example['unique_word_proportion'] = len(unique_words) / len(words) if words else 0\n",
    "        # Type-Token Ratio (TTR) via spaCy\n",
    "    doc = nlp(\" \".join(words))\n",
    "    unique_words = set(token.text for token in doc)\n",
    "    example['type_token_ratio'] = len(unique_words) / len(words) if words else 0\n",
    "\n",
    "    example['long_word_proportion'] = len(long_words) / len(words) if words else 0\n",
    "    example['imagery_word_proportion'] = len(imagery_words) / len(words) if words else 0\n",
    "\n",
    "    blob = TextBlob(example['full_text'])\n",
    "    example['positive_sentiment_score'] = blob.sentiment.polarity if blob.sentiment.polarity > 0 else 0\n",
    "    example['negative_sentiment_score'] = -blob.sentiment.polarity if blob.sentiment.polarity < 0 else 0\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(extract_stylistic_features)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual and Descriptive Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd860f3e6ec2447fa426a1629901699e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ea6c61b4ec419a86a411ce8ff4c0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf2685e57624923bae1d7997b692732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'essay_id': 'ea26dc4', 'full_text': 'I belive that they should change the elections to elections by popular vote for the president of the United States.\\n\\nThe electoral College consists of 530 electors, tey choose the president by a vote in congress or \"qualified\" citizens. A majoriy of 270 electoral votes are required to elect a president, therefore in my opinion a popula vote would have mor meaning, beacause it\\'s more people the vote would be chosen fr the mayority making the rest of the people satisfied. The number of citizens all togete versus the number of the electors at the Electoral College is superior; the people are te ones that follow the law might as well let them choose he president thats going to run the country and establish laws.\\n\\nThe electoral college system prevents us from voting for the presidet directly, instead they make us vote for a slate of electors, who in turn elect the pesident. For example if you were to live in Texas and wanted to vote for a president you\\'d have to vote for a slate of 34 elector of that political party that are pleged to him or her, on the off-chance that those electors won the statewide elections. The electors get picked at state conventions, the states party\\'s central committee and sometimes the presidential candidates themselves. Also electos can be anyone not holding public office.\\n\\nFurthermore the best argument against the electoral college is the disaster factor. Americans should consider themselves lucky that the 2000 disaster was he biggest crisis in elections that century since the system allows much worse. Consider the fact that the state legislatures are in a way responsible for piking electors and those elector could defy the will of the people back in 1960; what tells you that they stopped doing it?\\n\\nThe electoral college system has a lo of flaws and they can choose if they want o do what the peopl think is best or just take mater into their own hands and do as they plese.\\n\\nIn conclution its very clear that they should get rid of the electoral college and just let the peopl vote directly fo the president. That would cause less drama and would prevent fiascos in the elections.       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0\\xa0\\xa0\\xa0\\xa0     ', 'score': 3, 'unique_mistakes': 20.0, 'repeated_mistakes_count': 2.0, 'max_repeated_mistake': 2.0, 'word_count': 371.0, 'flesch_reading_ease': 61.5, 'flesch_kincaid_grade': 11.3, 'sentence_count': 14, 'average_sentence_length': 28.5, 'pos_noun_count': 95, 'pos_verb_count': 65, 'pos_adj_count': 27, 'pos_adv_count': 15, 'grammar_error_count': 30, 'syntactic_complexity': 27.285714285714285, 'spelling_mistake_count': 21, 'error_density': 0.13636363636363635, 'tfidf_keywords_vector': [0.1977982064113758, 0.042789746378051934, 0.6732435383791084, -0.6178010730898644, 0.08586393617102417, -0.0689542823154062, 0.00959424838752973, -0.00433862013677394, 0.015521928894408812, 0.028556710631481227, 0.003757644518672762, 0.04918226803064273, -0.010506208564133262, 0.048212915728345004, -0.029402427110320102, -0.009567565474337441, 0.13541366258786106, -0.030701949879952065, 0.03407629912825338, -0.011728986640566869, 0.04731903765236058, 0.024278565233402327, -0.0202055220873499, -0.00344041726757616, -0.0059983107793590154, -0.04064541259318033, 0.016375954682992234, -0.01135368585284111, 0.03860618222275983, 0.025629004680896985, -0.008082578985205964, -0.04885045629851237, 0.028809704988296735, -0.02570286456190781, 0.006637146444963989, 0.035600884341260595, 0.025131795491315403, -0.05378137159965886, 0.04980907506456732, -0.04061273791969073, 0.02232726496054869, 0.0870364054535328, 0.11083698930537171, -0.009844311301486856, 0.021896946448464568, -0.013370645226224857, -0.04935768852153144, 0.055487858614111533, 0.04341156952912014, -0.04025193578871404], 'lda_topic_vector': [0.00037597768823616207, 0.00037597730988636613, 0.011616736650466919, 0.00037597701884806156, 0.00037597701884806156, 0.00037597701884806156, 0.9190030097961426, 0.06058431416749954, 0.00037597757182084024, 0.000375977746443823, 0.0003759779501706362, 0.0003759773971978575, 0.004660164937376976, 0.00037597757182084024, 0.00037597748450934887], 'keyword_coverage': 0.28, 'pronoun_usage': 0.010638297872340425, 'unique_word_proportion': 0.6914893617021277, 'long_word_proportion': 0.32978723404255317, 'imagery_word_proportion': 0.13829787234042554, 'positive_sentiment_score': 0.3301443001443001, 'negative_sentiment_score': 0.0, 'visual_word_proportion': 0.31382978723404253, 'unique_visual_word_proportion': 0.22340425531914893, 'average_imagery_score': 1.4047619047619047}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def is_visual_word(word):\n",
    "    \"\"\"Checks if a word evokes visual imagery, using WordNet synsets.\"\"\"\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return False\n",
    "    \n",
    "    visual_tags = ['noun.artifact', 'noun.object', 'noun.plant', 'noun.animal', 'noun.body', 'adj.all']\n",
    "    return any(tag in str(synset.lexname()) for synset in synsets for tag in visual_tags)\n",
    "\n",
    "def extract_visual_descriptive_features(example):\n",
    "    words = [word for word in word_tokenize(example['full_text'].lower()) if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    visual_words = [word for word in words if is_visual_word(word)]\n",
    "    unique_visual_words = set(visual_words)\n",
    "\n",
    "    example['visual_word_proportion'] = len(visual_words) / len(words) if words else 0\n",
    "    example['unique_visual_word_proportion'] = len(unique_visual_words) / len(words) if words else 0\n",
    "    example['average_imagery_score'] = len(visual_words) / len(unique_visual_words) if unique_visual_words else 0\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(extract_visual_descriptive_features)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohesion and Coherence Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a403b40233415b9af53c725492bd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1a95d5290843e492b759467452bb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bb8e7b3dfb40c488f4fe50bc480679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'essay_id': 'ea26dc4', 'full_text': 'I belive that they should change the elections to elections by popular vote for the president of the United States.\\n\\nThe electoral College consists of 530 electors, tey choose the president by a vote in congress or \"qualified\" citizens. A majoriy of 270 electoral votes are required to elect a president, therefore in my opinion a popula vote would have mor meaning, beacause it\\'s more people the vote would be chosen fr the mayority making the rest of the people satisfied. The number of citizens all togete versus the number of the electors at the Electoral College is superior; the people are te ones that follow the law might as well let them choose he president thats going to run the country and establish laws.\\n\\nThe electoral college system prevents us from voting for the presidet directly, instead they make us vote for a slate of electors, who in turn elect the pesident. For example if you were to live in Texas and wanted to vote for a president you\\'d have to vote for a slate of 34 elector of that political party that are pleged to him or her, on the off-chance that those electors won the statewide elections. The electors get picked at state conventions, the states party\\'s central committee and sometimes the presidential candidates themselves. Also electos can be anyone not holding public office.\\n\\nFurthermore the best argument against the electoral college is the disaster factor. Americans should consider themselves lucky that the 2000 disaster was he biggest crisis in elections that century since the system allows much worse. Consider the fact that the state legislatures are in a way responsible for piking electors and those elector could defy the will of the people back in 1960; what tells you that they stopped doing it?\\n\\nThe electoral college system has a lo of flaws and they can choose if they want o do what the peopl think is best or just take mater into their own hands and do as they plese.\\n\\nIn conclution its very clear that they should get rid of the electoral college and just let the peopl vote directly fo the president. That would cause less drama and would prevent fiascos in the elections.       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0\\xa0\\xa0\\xa0\\xa0     ', 'score': 3, 'unique_mistakes': 20.0, 'repeated_mistakes_count': 2.0, 'max_repeated_mistake': 2.0, 'word_count': 371.0, 'flesch_reading_ease': 61.5, 'flesch_kincaid_grade': 11.3, 'sentence_count': 14, 'average_sentence_length': 28.5, 'pos_noun_count': 95, 'pos_verb_count': 65, 'pos_adj_count': 27, 'pos_adv_count': 15, 'grammar_error_count': 30, 'syntactic_complexity': 27.285714285714285, 'spelling_mistake_count': 21, 'error_density': 0.13636363636363635, 'tfidf_keywords_vector': [0.1977982064113758, 0.042789746378051934, 0.6732435383791084, -0.6178010730898644, 0.08586393617102417, -0.0689542823154062, 0.00959424838752973, -0.00433862013677394, 0.015521928894408812, 0.028556710631481227, 0.003757644518672762, 0.04918226803064273, -0.010506208564133262, 0.048212915728345004, -0.029402427110320102, -0.009567565474337441, 0.13541366258786106, -0.030701949879952065, 0.03407629912825338, -0.011728986640566869, 0.04731903765236058, 0.024278565233402327, -0.0202055220873499, -0.00344041726757616, -0.0059983107793590154, -0.04064541259318033, 0.016375954682992234, -0.01135368585284111, 0.03860618222275983, 0.025629004680896985, -0.008082578985205964, -0.04885045629851237, 0.028809704988296735, -0.02570286456190781, 0.006637146444963989, 0.035600884341260595, 0.025131795491315403, -0.05378137159965886, 0.04980907506456732, -0.04061273791969073, 0.02232726496054869, 0.0870364054535328, 0.11083698930537171, -0.009844311301486856, 0.021896946448464568, -0.013370645226224857, -0.04935768852153144, 0.055487858614111533, 0.04341156952912014, -0.04025193578871404], 'lda_topic_vector': [0.00037597768823616207, 0.00037597730988636613, 0.011616736650466919, 0.00037597701884806156, 0.00037597701884806156, 0.00037597701884806156, 0.9190030097961426, 0.06058431416749954, 0.00037597757182084024, 0.000375977746443823, 0.0003759779501706362, 0.0003759773971978575, 0.004660164937376976, 0.00037597757182084024, 0.00037597748450934887], 'keyword_coverage': 0.28, 'pronoun_usage': 0.010638297872340425, 'unique_word_proportion': 0.6914893617021277, 'long_word_proportion': 0.32978723404255317, 'imagery_word_proportion': 0.13829787234042554, 'positive_sentiment_score': 0.3301443001443001, 'negative_sentiment_score': 0.0, 'visual_word_proportion': 0.31382978723404253, 'unique_visual_word_proportion': 0.22340425531914893, 'average_imagery_score': 1.4047619047619047, 'discourse_marker_count': 0.09510869565217392, 'neural_coherence_score': 0.48606157302856445}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# RUN python -m spacy download en_core_web_sm first to download the model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_cohesion_features(example):\n",
    "    \n",
    "    # Discourse Coherence with Dependency Parsing\n",
    "    doc = nlp(example['full_text'])\n",
    "    discourse_marker_count = 0\n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"mark\", \"cc\", \"advmod\"}:  \n",
    "            discourse_marker_count += 1\n",
    "\n",
    "    words = [token.text for token in doc if token.is_alpha]\n",
    "    example['discourse_marker_count'] = discourse_marker_count / len(words) if words else 0\n",
    "\n",
    "    # Neural Coherence Score\n",
    "    sentences = sent_tokenize(example['full_text'])\n",
    "    if len(sentences) > 1:\n",
    "        sentence_embeddings = embedding_model.encode(sentences)\n",
    "        similarities = [\n",
    "            np.dot(sentence_embeddings[i], sentence_embeddings[i + 1]) / \n",
    "            (np.linalg.norm(sentence_embeddings[i]) * np.linalg.norm(sentence_embeddings[i + 1]))\n",
    "            for i in range(len(sentence_embeddings) - 1)\n",
    "        ]\n",
    "        example['neural_coherence_score'] = np.mean(similarities) if similarities else 0\n",
    "    else:\n",
    "        example['neural_coherence_score'] = 0  \n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(extract_cohesion_features)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c9b9a4d8cf49e69c587eff946907de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/803 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df5e1437e0747adbd7c95d2f8854376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075eb34d372646399f4e8fddc5022761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e1c59dc8e2420e9916011fd333e937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053b6403c8584521bff01b5508f43657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25db21105a0a45679830baf161e06100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b731437da1c44fee8c24b71af4e08747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d57871e8be34a0cb37c9e679c4fe3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'essay_id': 'ea26dc4', 'full_text': 'I belive that they should change the elections to elections by popular vote for the president of the United States.\\n\\nThe electoral College consists of 530 electors, tey choose the president by a vote in congress or \"qualified\" citizens. A majoriy of 270 electoral votes are required to elect a president, therefore in my opinion a popula vote would have mor meaning, beacause it\\'s more people the vote would be chosen fr the mayority making the rest of the people satisfied. The number of citizens all togete versus the number of the electors at the Electoral College is superior; the people are te ones that follow the law might as well let them choose he president thats going to run the country and establish laws.\\n\\nThe electoral college system prevents us from voting for the presidet directly, instead they make us vote for a slate of electors, who in turn elect the pesident. For example if you were to live in Texas and wanted to vote for a president you\\'d have to vote for a slate of 34 elector of that political party that are pleged to him or her, on the off-chance that those electors won the statewide elections. The electors get picked at state conventions, the states party\\'s central committee and sometimes the presidential candidates themselves. Also electos can be anyone not holding public office.\\n\\nFurthermore the best argument against the electoral college is the disaster factor. Americans should consider themselves lucky that the 2000 disaster was he biggest crisis in elections that century since the system allows much worse. Consider the fact that the state legislatures are in a way responsible for piking electors and those elector could defy the will of the people back in 1960; what tells you that they stopped doing it?\\n\\nThe electoral college system has a lo of flaws and they can choose if they want o do what the peopl think is best or just take mater into their own hands and do as they plese.\\n\\nIn conclution its very clear that they should get rid of the electoral college and just let the peopl vote directly fo the president. That would cause less drama and would prevent fiascos in the elections.       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0\\xa0\\xa0\\xa0\\xa0     ', 'score': 3, 'unique_mistakes': 20.0, 'repeated_mistakes_count': 2.0, 'max_repeated_mistake': 2.0, 'word_count': 371.0, 'flesch_reading_ease': 61.5, 'flesch_kincaid_grade': 11.3, 'sentence_count': 14, 'average_sentence_length': 28.5, 'pos_noun_count': 95, 'pos_verb_count': 65, 'pos_adj_count': 27, 'pos_adv_count': 15, 'grammar_error_count': 30, 'syntactic_complexity': 27.285714285714285, 'spelling_mistake_count': 21, 'error_density': 0.13636363636363635, 'tfidf_keywords_vector': [0.1977982064113758, 0.042789746378051934, 0.6732435383791084, -0.6178010730898644, 0.08586393617102417, -0.0689542823154062, 0.00959424838752973, -0.00433862013677394, 0.015521928894408812, 0.028556710631481227, 0.003757644518672762, 0.04918226803064273, -0.010506208564133262, 0.048212915728345004, -0.029402427110320102, -0.009567565474337441, 0.13541366258786106, -0.030701949879952065, 0.03407629912825338, -0.011728986640566869, 0.04731903765236058, 0.024278565233402327, -0.0202055220873499, -0.00344041726757616, -0.0059983107793590154, -0.04064541259318033, 0.016375954682992234, -0.01135368585284111, 0.03860618222275983, 0.025629004680896985, -0.008082578985205964, -0.04885045629851237, 0.028809704988296735, -0.02570286456190781, 0.006637146444963989, 0.035600884341260595, 0.025131795491315403, -0.05378137159965886, 0.04980907506456732, -0.04061273791969073, 0.02232726496054869, 0.0870364054535328, 0.11083698930537171, -0.009844311301486856, 0.021896946448464568, -0.013370645226224857, -0.04935768852153144, 0.055487858614111533, 0.04341156952912014, -0.04025193578871404], 'lda_topic_vector': [0.00037597768823616207, 0.00037597730988636613, 0.011616736650466919, 0.00037597701884806156, 0.00037597701884806156, 0.00037597701884806156, 0.9190030097961426, 0.06058431416749954, 0.00037597757182084024, 0.000375977746443823, 0.0003759779501706362, 0.0003759773971978575, 0.004660164937376976, 0.00037597757182084024, 0.00037597748450934887], 'keyword_coverage': 0.28, 'pronoun_usage': 0.010638297872340425, 'unique_word_proportion': 0.6914893617021277, 'long_word_proportion': 0.32978723404255317, 'imagery_word_proportion': 0.13829787234042554, 'positive_sentiment_score': 0.3301443001443001, 'negative_sentiment_score': 0.0, 'visual_word_proportion': 0.31382978723404253, 'unique_visual_word_proportion': 0.22340425531914893, 'average_imagery_score': 1.4047619047619047, 'discourse_marker_count': 0.09510869565217392, 'neural_coherence_score': 0.48606157302856445, 'longformer_sentence_embedding': [0.010260667651891708, 0.16882109642028809, -0.21883146464824677, -0.1291627436876297, -0.0067739104852080345, 0.006054803263396025, -0.09153050929307938, 0.022464685142040253, -0.01959330029785633, -0.010730051435530186, 0.2683364152908325, 0.06391146034002304, -0.1576027125120163, 0.1511964350938797, 0.06022452935576439, -0.1171572133898735, 0.08977240324020386, -0.02455991506576538, 0.26932790875434875, -0.17927931249141693, 0.013474845327436924, 0.139310821890831, 0.1889127790927887, 0.012442014180123806, -0.13470526039600372, -0.3147561848163605, -0.10720876604318619, -0.04819650202989578, 0.056917086243629456, 0.06033063679933548, -0.30468183755874634, -0.029371414333581924, 0.06522785127162933, -0.16922974586486816, 0.11066664755344391, -0.034019555896520615, -0.1532660275697708, -0.11314812302589417, -0.014977234415709972, -0.057627808302640915, 0.12745915353298187, 0.15302029252052307, -0.02507117949426174, -0.08531353622674942, 0.21142083406448364, 0.036467790603637695, -0.0006254192558117211, 0.03830844908952713, 0.0841178372502327, -0.06805834919214249, -0.0762878805398941, -0.5629207491874695, 0.12345463782548904, -0.3833685517311096, -0.1049659475684166, 0.06355395168066025, 0.11046134680509567, -0.017757629975676537, 0.07501663267612457, 0.05952886864542961, -0.15430517494678497, 0.05137605965137482, 0.1031811386346817, -0.010222058743238449, 0.1122002825140953, 0.11235947906970978, 0.0965844914317131, -0.1464885175228119, 0.16932015120983124, 0.16301508247852325, -0.45134660601615906, -0.024823172017931938, -0.18339066207408905, 0.11795470118522644, -0.00022711855126544833, -0.055410586297512054, -0.02591530978679657, 0.0922698974609375, 0.12481561303138733, 0.21122261881828308, -0.013401903212070465, -0.11989456415176392, -0.15662890672683716, 0.20577405393123627, -0.2663794755935669, 0.07905232906341553, 0.026673799380660057, -0.05752831697463989, 0.060809697955846786, 0.03127230703830719, -0.07923275232315063, 0.6220992803573608, 0.13477736711502075, -0.48233795166015625, 0.07771042734384537, -0.06760243326425552, -0.26717251539230347, 0.3053252398967743, -0.06210387125611305, -0.08046292513608932, -0.07607144117355347, -0.006656748242676258, 0.14711402356624603, 0.3173940181732178, -0.03072039596736431, -0.05261211842298508, -0.1659541130065918, -0.041315771639347076, -0.21174608170986176, 0.1552959829568863, 0.037548501044511795, -0.06853830814361572, 0.021087970584630966, -0.24110199511051178, -0.1935568004846573, -0.08170425146818161, 0.14606423676013947, 0.06430669873952866, 0.08016210049390793, 0.07022902369499207, 0.004352745600044727, -0.03637145087122917, 0.08483030647039413, -0.032217495143413544, 0.21813899278640747, -0.007137552369385958, -0.00794551707804203, -0.27282455563545227, 0.10508018732070923, -0.05335799604654312, -0.29627296328544617, -0.10558921098709106, 0.07651432603597641, 0.08506409078836441, -0.023019937798380852, -0.2740038335323334, 0.2040645331144333, 0.027353325858712196, -0.3431289792060852, 0.02313559502363205, -0.05823288485407829, -0.08986503630876541, -0.29330605268478394, 0.12290416657924652, -0.07076048851013184, -0.46183276176452637, -0.12633395195007324, -0.051756393164396286, 0.13474217057228088, -0.0655655786395073, -0.07317899167537689, 0.04621636122465134, 0.18790481984615326, -0.1373787373304367, 0.04551701620221138, 0.052648238837718964, 0.0971563383936882, 0.07287732511758804, 0.035120364278554916, -0.01834688149392605, 0.0782802477478981, -0.044561032205820084, -0.18707619607448578, 0.01901470310986042, -0.03126932680606842, 0.04274377226829529, -0.01591513119637966, 0.21838994324207306, -0.0735110193490982, 0.05169326812028885, -0.2742536664009094, 0.15936189889907837, 0.009895512834191322, 0.213689386844635, -0.08772645145654678, -0.11956661939620972, -0.03797179087996483, -0.022955745458602905, -0.011930818669497967, 0.15036626160144806, 0.01935095526278019, 0.02351146563887596, 0.010362767614424229, 0.17156006395816803, 0.20488297939300537, -0.1022079810500145, 0.016216279938817024, -0.09771543741226196, -0.23948386311531067, 0.03276102617383003, 0.24701614677906036, -0.16023945808410645, -0.03812326118350029, -0.04176416993141174, 0.10776501148939133, -0.04271741211414337, -0.0854683667421341, -0.03474421426653862, -0.12089954316616058, 0.055016156286001205, -0.09649280458688736, -0.2828304171562195, 0.2585260570049286, -0.20414148271083832, 0.3004623055458069, 0.2613234221935272, 0.05590561777353287, -0.09556647390127182, -0.03041507862508297, 0.05088420957326889, -0.020121807232499123, 0.18444474041461945, 0.09202149510383606, -0.01820983737707138, 0.0023034822661429644, -0.21258459985256195, 0.21125927567481995, 0.16718392074108124, 0.14769160747528076, -0.047420233488082886, 0.06693380326032639, 0.06500919908285141, 0.17797406017780304, 0.15618452429771423, -0.16119742393493652, -0.2932521402835846, -0.08230694383382797, -0.01817036047577858, 0.09796319156885147, -0.11990806460380554, -0.03077751211822033, -0.03692671284079552, 0.10963819175958633, 0.14349035918712616, 0.18796682357788086, -0.25846225023269653, -0.09596362709999084, -0.17517812550067902, 0.0838547796010971, 0.015545299276709557, -0.18862059712409973, -0.002108813961967826, 0.014672618359327316, 0.22232574224472046, -0.01209200918674469, -0.15730348229408264, -0.08975055068731308, 0.061205897480249405, 0.06612785905599594, 0.20482942461967468, 0.13065844774246216, 0.1078638955950737, 0.5071642398834229, -0.06808330863714218, 0.0942203626036644, 0.12035279721021652, 0.3008960783481598, -0.2507164776325226, -0.16740988194942474, 0.011112698353827, -0.17458224296569824, -0.0020040522795170546, 0.14108358323574066, -0.1279088705778122, 0.22287388145923615, 0.09501723200082779, 0.2926809787750244, -0.04670146480202675, 0.05367767810821533, -0.1511376053094864, -0.15649142861366272, -0.06671327352523804, -0.0355241484940052, 0.11253497004508972, 0.25294414162635803, 0.23737336695194244, 0.2181679755449295, -0.017251798883080482, 0.023653356358408928, 0.0006586442468687892, 0.8223810195922852, 0.051913704723119736, 0.11513516306877136, 0.08342766016721725, -0.11215769499540329, -0.26885324716567993, 0.13300445675849915, -0.0025747977197170258, -0.25877806544303894, 0.04064461961388588, -0.09827691316604614, -0.08732125163078308, 0.1434645652770996, 0.20910122990608215, 0.058788519352674484, -0.19930246472358704, 0.19034895300865173, -0.05715567618608475, 0.20302370190620422, -0.008669320493936539, -0.15372991561889648, -0.07020817697048187, -0.047805190086364746, -0.2533835172653198, 0.008785838261246681, -5.487411908688955e-05, 0.021194368600845337, -0.16598257422447205, -0.0405011847615242, -0.04542655125260353, 0.05004904046654701, -0.11940237879753113, -0.012267161160707474, 0.016694387421011925, -0.03274234011769295, -0.07443602383136749, 0.06374870985746384, -0.0422617606818676, -0.36957067251205444, -0.2512987554073334, -0.10677137970924377, -0.025804420933127403, -0.04347031190991402, -0.022652646526694298, -0.13373684883117676, -0.11407756060361862, -0.12952211499214172, 0.20279519259929657, -0.10040357708930969, 0.03342349827289581, -0.22935162484645844, -0.013726620934903622, -0.007941629737615585, 0.16517169773578644, -0.2529391348361969, 0.3193512558937073, 0.16681300103664398, -0.02040157839655876, -0.027976352721452713, -0.04144473746418953, 0.061260368674993515, -0.11454788595438004, 0.12475848197937012, -0.0016041129129007459, -0.1687035709619522, -0.08354787528514862, -0.16744756698608398, -0.12376821786165237, -0.1306765079498291, -0.07326426357030869, 0.12193695455789566, -0.14608871936798096, 0.13593940436840057, -0.13695988059043884, 0.20548726618289948, 0.16003313660621643, 0.23584143817424774, 0.1680999994277954, 0.04528772458434105, 0.08314734697341919, -0.13439695537090302, -0.004309542942792177, 0.25598037242889404, -0.17424623668193817, -0.08127406984567642, 0.010511954315006733, 0.10479804128408432, 0.20521973073482513, 0.05665621533989906, 0.19876541197299957, 0.1755368709564209, 0.16059796512126923, -0.13826322555541992, 0.13410499691963196, 0.027943970635533333, -0.07005047053098679, -0.017546143382787704, -0.10127290338277817, -0.1789172738790512, 0.01376769132912159, 0.13772538304328918, -0.017586447298526764, -0.1385832577943802, 0.07887126505374908, 0.13164667785167694, -0.2217596173286438, 0.11748327314853668, -0.08870429545640945, 0.3438872694969177, 0.01294618472456932, -0.02588278241455555, 0.09090214967727661, 0.08248906582593918, -0.13654391467571259, -0.08650582283735275, -0.014545467682182789, 0.06567617505788803, -0.1235627755522728, 0.1088862195611, 0.9336188435554504, -0.19343793392181396, -0.16647681593894958, 0.33034589886665344, -0.290813148021698, 0.019372792914509773, 0.09334146976470947, 0.10878981649875641, -0.11200283467769623, 0.08611942827701569, -0.15804453194141388, 0.01209633331745863, 0.2861807346343994, -0.13011996448040009, -0.13265912234783173, -0.12439871579408646, -0.2645677328109741, 0.04444298520684242, -0.012343235313892365, 0.13299600780010223, 0.07722540199756622, -0.05893872678279877, -0.25334441661834717, -0.05859442055225372, -0.051864489912986755, -0.3433629274368286, 0.030006587505340576, -0.03552776202559471, 0.032013893127441406, -0.11781466007232666, -0.1466389000415802, 0.07724659144878387, -0.012894261628389359, -0.20307496190071106, -0.10654960572719574, -0.07718279957771301, -0.0343707874417305, -0.20949113368988037, -0.304636687040329, 0.035749878734350204, -0.07287770509719849, 0.1516127586364746, 0.13369311392307281, 0.03647521510720253, -0.041935477405786514, -0.03763877972960472, -0.05894804745912552, 0.05393693968653679, -0.0017773737199604511, -0.16404704749584198, 0.04590784013271332, 0.007976570166647434, 0.032951194792985916, -0.1907966583967209, -0.11290570348501205, -0.08593028038740158, -0.07022018730640411, 0.19136464595794678, -0.0905834436416626, -0.08329564332962036, 0.2129708230495453, -0.3689511716365814, -0.21263356506824493, 0.04948273301124573, 0.1081419512629509, -0.06330371648073196, -0.1007925420999527, -0.3594668209552765, 0.1298283189535141, -0.09042029827833176, -0.02758435346186161, 0.5639581084251404, 0.13755880296230316, -0.16797399520874023, -0.09060477465391159, -0.08113183081150055, 0.0041146958246827126, -0.11378441751003265, 0.14907006919384003, 0.14383870363235474, -0.19829480350017548, 0.09127239882946014, -0.03537936508655548, 0.17836208641529083, 0.055036820471286774, -0.2782982289791107, 0.32729804515838623, 0.06866440176963806, -0.03782402351498604, -0.2058778554201126, 0.07348892837762833, 0.03241322562098503, 0.14497195184230804, -0.12768149375915527, -0.028822684660553932, 0.03689161688089371, -0.06437663733959198, -0.04021923243999481, -0.02428949438035488, 0.27153459191322327, 0.28342404961586, 0.1462751179933548, 0.0002665205392986536, 0.14340727031230927, -0.2655709385871887, -0.04260905832052231, 0.08770487457513809, -0.22060680389404297, 0.05165134370326996, -0.11218035966157913, -0.050115104764699936, -0.01783401146531105, 0.26708096265792847, -0.10258549451828003, -0.10276412218809128, -0.22428879141807556, 0.10855196416378021, -0.17904451489448547, 0.01853545941412449, 0.14974267780780792, 0.1847662627696991, -0.0892055481672287, 0.056584347039461136, -0.1334463506937027, -0.13759362697601318, 0.0783378854393959, 0.0016333645908161998, -0.060134682804346085, 0.04613995552062988, 0.036218781024217606, -0.015210294164717197, -0.05756688863039017, -0.13695091009140015, -0.12958906590938568, -0.008327837102115154, -0.12023072689771652, -0.21127641201019287, 0.012439441867172718, 0.06400956213474274, 0.026579339057207108, 0.6841341257095337, -0.23349443078041077, 0.12804406881332397, 0.0917486697435379, -0.2507249116897583, -0.07759235054254532, 0.07994842529296875, -0.17535845935344696, 2.4663140773773193, 0.2768471837043762, -0.1282585859298706, -0.1799924075603485, 0.14827610552310944, 0.010056964121758938, -0.14261309802532196, 0.03860849887132645, -0.0570247508585453, -0.03586630895733833, 0.28763020038604736, -0.08685767650604248, -0.07719553261995316, 0.15940889716148376, 0.23650556802749634, 0.12513454258441925, -0.07249124348163605, -0.10882647335529327, 0.10484711080789566, -0.09371049702167511, -0.004292344208806753, 0.25303593277931213, -0.08581388741731644, 0.2063896358013153, 0.05199887603521347, -0.30797111988067627, -0.27633780241012573, 0.19499722123146057, -0.05769418925046921, -0.07486383616924286, 0.05247543007135391, -0.024165159091353416, 0.01278380211442709, -0.033747509121894836, -0.0640806183218956, -0.07429346442222595, 0.05460992082953453, -0.08704753965139389, -0.006148656364530325, -0.17101836204528809, 0.021081654354929924, -0.05086095258593559, -0.10909821093082428, 0.10977842658758163, -0.23426435887813568, 0.19361667335033417, -0.14533917605876923, -0.04282636567950249, -0.08971720188856125, 0.06684800237417221, 0.07597657293081284, 0.1283709555864334, 0.07662363350391388, -0.008523192256689072, 0.16356922686100006, 0.05553753674030304, 0.09959486871957779, 0.009584134444594383, 0.1795807033777237, -0.1303420066833496, -0.01934514194726944, 0.2795911133289337, 0.007433835417032242, -0.39303579926490784, 0.05042767524719238, 0.07476329803466797, 0.13598507642745972, 0.008139844052493572, -0.0458354614675045, 0.509682834148407, -0.002422744408249855, 0.039697762578725815, -0.12037722766399384, 0.237367182970047, -0.2720668613910675, -0.25113382935523987, -0.1841195821762085, 0.06582484394311905, -0.14785952866077423, 0.16726940870285034, -0.0893755555152893, 0.2356775552034378, -0.22796577215194702, 0.06441246718168259, -28.232486724853516, 0.0066519007086753845, 0.028591753914952278, -0.21497006714344025, 0.15525422990322113, -0.12348737567663193, -0.19906723499298096, -0.06631497293710709, -0.18375927209854126, 0.007503541186451912, 0.542884349822998, -0.01346952561289072, -0.014847629703581333, 0.006109889596700668, -0.019416728988289833, 0.025546537712216377, -0.006396694574505091, -0.1272166520357132, 0.05202428996562958, -0.17705385386943817, 0.07277414947748184, -0.11562308669090271, 0.0211695097386837, 0.05224866792559624, 0.04348745197057724, 0.31167101860046387, 0.06377740949392319, 0.08468632400035858, 0.11529594659805298, -0.012087215669453144, -0.06466089934110641, -0.029560083523392677, 0.10562922805547714, -0.026919180527329445, 0.17424899339675903, 0.09418024122714996, -0.09500373154878616, 0.1195695698261261, -0.039807941764593124, 0.02086004801094532, -0.04250044375658035, 0.02143380604684353, -2.6831603050231934, -0.002450440777465701, 0.04253726825118065, -0.005567024927586317, -0.0014569914201274514, -0.20520706474781036, 0.014896240085363388, -0.04517202079296112, 0.16252894699573517, 0.15934601426124573, -0.01568078249692917, -0.03615973889827728, -3.4381002478767186e-05, 0.22676560282707214, 0.02820165455341339, 0.07736289501190186, -0.14297090470790863, 0.21418964862823486, 0.20901846885681152, -0.00031533034052699804, 0.09614523500204086, -0.21896325051784515, 0.012838167138397694, -0.1627490371465683, -0.1489911526441574, -0.15768152475357056, 0.008499251678586006, 0.04290952533483505, 0.04893866926431656, 0.017628077417612076, -0.09320651739835739, 0.15384045243263245, -0.012487228959798813, -0.006941957864910364, -0.09609805047512054, -0.03452880680561066, -0.08498268574476242, 0.004198398441076279, -0.13428638875484467, -0.06800708919763565, 0.08262807875871658, 0.20796583592891693, 0.12982702255249023, 0.11321090906858444, 0.09421706199645996, -0.054050784558057785, 0.146656334400177, -0.04545840993523598, 0.1845833957195282, -0.08525272458791733, 0.029502570629119873, 0.05997125059366226, -0.05802389979362488, -0.16508057713508606, -0.13383996486663818, -0.13879497349262238, 0.1184222474694252, 0.0028129012789577246, 0.036668021231889725, 0.024117818102240562, 0.02177785523235798, 0.05776185914874077, 0.046513881534338, 0.06284613162279129, 0.2743074595928192, 0.1430111676454544, 0.1247033178806305, -0.08877328038215637, 0.19564189016819, 0.2025013417005539, 0.2101145088672638, -0.20286354422569275, -0.12485615164041519, -0.01921244151890278, 0.12030057609081268, -0.15691830217838287, -0.1664305478334427, -0.06957080960273743, -0.11542084813117981, -0.29390111565589905, 0.191878080368042, 0.12186385691165924, 0.25385186076164246, 0.03548210486769676, -0.24598431587219238, -0.16268131136894226, 0.1942574679851532, 0.11054209619760513, 0.11360026895999908, -0.09416596591472626, 0.22915536165237427, -0.04346916452050209, 0.2263314127922058, 0.003154555568471551, -0.04726221039891243, -0.17836979031562805, -0.060578420758247375, -0.09491179883480072, -0.012911021709442139, 0.07856666296720505, 0.08460745960474014, -0.11781331151723862, 0.11018329858779907, -0.16949708759784698, -0.2581402659416199, -0.18774820864200592, -0.2004067301750183, -0.10166777670383453, 0.015889329835772514, -0.21227441728115082, 0.12312952429056168, 0.30731192231178284, -0.15483543276786804, -0.2995954155921936, 0.06553465127944946, -0.2267838567495346, -0.032973650842905045, 0.15889853239059448, 0.03321140259504318, 0.04766329005360603, 0.09486328810453415, 0.2579379379749298, -0.07226387411355972, 0.017777059227228165, -0.01465363148599863, -0.12000641971826553, 0.06524854898452759, 0.07035654038190842, 0.03866245225071907, -0.15461312234401703, -0.053987544029951096, 0.05998992919921875, 0.10280711203813553, 0.03225240483880043, 0.18385344743728638, 0.11123815923929214, 0.09310846775770187, -0.047823335975408554, 0.1054786741733551, -0.10868437588214874, 0.027525732293725014, -0.052922800183296204, 0.011374427936971188, -0.03904023393988609, -0.021542992442846298, -0.11826840043067932, 0.05422366410493851, -0.18155351281166077, -0.06678900867700577, -0.18593323230743408, 0.09973553568124771, 0.047262199223041534, 0.044428691267967224, 0.6066184043884277, -0.10982775688171387, -0.06267587840557098, 0.24398751556873322, -0.07522492855787277, -0.04973306506872177, 0.16805052757263184, -0.04546407610177994, -0.008023438975214958, 0.09656720608472824, -0.23839086294174194, -0.1208762601017952, 0.11961527168750763, 0.0984446257352829, 0.1257776916027069, -0.35781851410865784, 0.320469468832016, -0.05959388613700867, 0.025979844853281975, 0.014959138818085194, -0.10194606333971024, -0.09820623695850372, -0.13493402302265167, -0.10652265697717667, -0.14470990002155304, -0.022163577377796173, 0.006753250025212765, 0.02526100166141987, -0.15256726741790771, -0.09142608195543289, -0.05133838206529617, 0.08635801821947098, -0.15705761313438416, -0.15261338651180267, 0.11406267434358597, 0.04215129464864731, -0.1073634997010231, 0.07522553950548172, -0.08208384364843369, -0.021108832210302353, -0.07396036386489868, 0.05692166090011597, 0.0752323567867279, -0.18143188953399658, -0.031435612589120865, -0.15871372818946838, -0.18731951713562012, -0.26368606090545654, 0.07635198533535004, -0.002087509958073497, 0.1495896726846695, 0.0005973329534754157, -0.04109160229563713, -0.15682242810726166, -0.06326309591531754, 0.06448639929294586, -0.08549370616674423, -0.10300644487142563, 0.2370884120464325, -0.09181814640760422, 0.009364155121147633, -0.04714225232601166, -0.0707104280591011, -0.047534991055727005, -0.026498649269342422, 0.24683690071105957, 0.014786848798394203, 0.1341918557882309, 0.03120741993188858, 0.11649250239133835, -0.021418239921331406, 0.07118343561887741, -0.10776554048061371, 0.06423524022102356, 0.0486736036837101, -0.16551315784454346, -0.18839503824710846, 0.051693446934223175, 0.0012564458884298801, 0.18936100602149963, -0.021114246919751167, 0.013699641451239586, -0.03332342579960823, 0.06158028170466423, 0.12084215879440308, 0.017902212217450142, 0.03420983627438545, -0.025597400963306427, 0.9563150405883789, 0.0466313399374485, -0.007814156822860241, -0.12020917236804962, 0.11716967821121216, -0.15279120206832886, -0.13285502791404724, -0.19887840747833252, 0.08021210134029388, -0.01286983024328947, 0.03189748898148537, -0.0008485043072141707, 0.21130548417568207, 0.02131708711385727, 0.19320900738239288, -0.014085755683481693, -0.0008199546718969941, -0.017537560313940048, -0.00015168719983194023, -0.12365429848432541, -0.03907126560807228, 0.0233884509652853, -0.2593027353286743, 0.18899525701999664, -0.13246305286884308, 0.173058420419693, -0.38595810532569885, -0.16921159625053406, -0.17526446282863617, -0.08028191328048706, -0.013279936276376247, 0.018351128324866295, -0.06779544055461884, -0.09913279861211777, 0.1663038432598114, 0.1280367374420166, -0.11703798174858093, 0.032739702612161636, -0.2767750918865204, 0.07737113535404205, -0.1002732664346695, -0.28531011939048767, 0.061184871941804886, -0.15541011095046997, 0.036419063806533813, 0.13658297061920166, -0.1095634251832962, -0.041542522609233856, -0.15042658150196075, 0.1337975561618805, 0.06447510421276093, -0.07166528701782227, -0.3744516968727112, 0.008448264561593533, 0.07517343014478683, -0.09506021440029144, -0.04594746604561806, -0.07332299649715424, 0.039238687604665756, -0.32228103280067444, 0.024029945954680443, -0.15551437437534332, -0.16701510548591614, -0.04737086966633797, 0.011612586677074432, 0.237986221909523, 0.04214891046285629, -0.09996321052312851, -0.12207236140966415, -0.10468893498182297, 0.22824731469154358, -0.026490265503525734, -0.2995226979255676, -0.016882695257663727, -0.12228219956159592, 0.04623860493302345, 0.09085183590650558, -0.16225716471672058, -0.22379548847675323, 0.13956855237483978, 0.03528788313269615, 0.014492006972432137, 0.16914159059524536, 0.11929662525653839, -0.08804044127464294, -0.12134808301925659, -0.3378337621688843, 0.24374711513519287, -0.10789287090301514, 0.08722864091396332, -0.03205835446715355, -0.05175117775797844, 0.05890451371669769, -0.25189992785453796, 0.15282505750656128, 0.16745588183403015, -0.04489804431796074, 0.005501072853803635, -0.041075803339481354, 0.13084208965301514, -0.23985815048217773, 0.20621001720428467, -0.09158127754926682, -0.12226220220327377, 0.15662351250648499, -0.0667230561375618, 0.13415241241455078, -0.21382005512714386, -0.17844001948833466, 0.32537174224853516], 'longformer_coherence_score': 0.9929037094116211}\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load Longformer model and tokenizer, supporting sequences up to 4096 tokens\n",
    "longformer_model = LongformerModel.from_pretrained(\"allenai/longformer-large-4096\")\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-large-4096\")\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "longformer_model.to(device)\n",
    "\n",
    "# Function to get Longformer sentence embeddings\n",
    "def get_longformer_embedding(text):\n",
    "    # Tokenize the text and move input tensors to the same device as the model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = longformer_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Move to CPU for numpy compatibility\n",
    "    return embeddings\n",
    "\n",
    "# Function to calculate coherence score using cosine similarity between sentence embeddings\n",
    "def calculate_coherence_score(text):\n",
    "    sentences = text.split('.')\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    # Get Longformer embeddings for each sentence\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip():  # Avoid empty sentences\n",
    "            embedding = get_longformer_embedding(sentence.strip())\n",
    "            sentence_embeddings.append(embedding)\n",
    "\n",
    "    # Calculate pairwise cosine similarities between consecutive sentence embeddings\n",
    "    coherence_scores = [\n",
    "        cosine_similarity([sentence_embeddings[i]], [sentence_embeddings[i + 1]])[0][0]\n",
    "        for i in range(len(sentence_embeddings) - 1)\n",
    "    ]\n",
    "\n",
    "    return np.mean(coherence_scores) if coherence_scores else 0\n",
    "\n",
    "# Function to extract neural network-based features\n",
    "def extract_neural_features(example):\n",
    "    essay_embedding = get_longformer_embedding(example['full_text'])\n",
    "    example['longformer_sentence_embedding'] = essay_embedding\n",
    "\n",
    "    example['longformer_coherence_score'] = calculate_coherence_score(example['full_text'])\n",
    "\n",
    "    return example\n",
    "\n",
    "# Apply feature extraction to the dataset\n",
    "dataset = dataset.map(extract_neural_features)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Sophistication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59d1661a49d4cda94433602029b6c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cb92677efc43b29b61b360762ebefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789bd21d4a9c4c40a625fae233d17eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'essay_id': 'ea26dc4', 'full_text': 'I belive that they should change the elections to elections by popular vote for the president of the United States.\\n\\nThe electoral College consists of 530 electors, tey choose the president by a vote in congress or \"qualified\" citizens. A majoriy of 270 electoral votes are required to elect a president, therefore in my opinion a popula vote would have mor meaning, beacause it\\'s more people the vote would be chosen fr the mayority making the rest of the people satisfied. The number of citizens all togete versus the number of the electors at the Electoral College is superior; the people are te ones that follow the law might as well let them choose he president thats going to run the country and establish laws.\\n\\nThe electoral college system prevents us from voting for the presidet directly, instead they make us vote for a slate of electors, who in turn elect the pesident. For example if you were to live in Texas and wanted to vote for a president you\\'d have to vote for a slate of 34 elector of that political party that are pleged to him or her, on the off-chance that those electors won the statewide elections. The electors get picked at state conventions, the states party\\'s central committee and sometimes the presidential candidates themselves. Also electos can be anyone not holding public office.\\n\\nFurthermore the best argument against the electoral college is the disaster factor. Americans should consider themselves lucky that the 2000 disaster was he biggest crisis in elections that century since the system allows much worse. Consider the fact that the state legislatures are in a way responsible for piking electors and those elector could defy the will of the people back in 1960; what tells you that they stopped doing it?\\n\\nThe electoral college system has a lo of flaws and they can choose if they want o do what the peopl think is best or just take mater into their own hands and do as they plese.\\n\\nIn conclution its very clear that they should get rid of the electoral college and just let the peopl vote directly fo the president. That would cause less drama and would prevent fiascos in the elections.       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0       \\xa0\\xa0\\xa0\\xa0\\xa0     ', 'score': 3, 'unique_mistakes': 20.0, 'repeated_mistakes_count': 2.0, 'max_repeated_mistake': 2.0, 'word_count': 371.0, 'flesch_reading_ease': 61.5, 'flesch_kincaid_grade': 11.3, 'sentence_count': 14, 'average_sentence_length': 28.5, 'pos_noun_count': 95, 'pos_verb_count': 65, 'pos_adj_count': 27, 'pos_adv_count': 15, 'grammar_error_count': 30, 'syntactic_complexity': 27.285714285714285, 'spelling_mistake_count': 21, 'error_density': 0.13636363636363635, 'tfidf_keywords_vector': [0.1977982064113758, 0.042789746378051934, 0.6732435383791084, -0.6178010730898644, 0.08586393617102417, -0.0689542823154062, 0.00959424838752973, -0.00433862013677394, 0.015521928894408812, 0.028556710631481227, 0.003757644518672762, 0.04918226803064273, -0.010506208564133262, 0.048212915728345004, -0.029402427110320102, -0.009567565474337441, 0.13541366258786106, -0.030701949879952065, 0.03407629912825338, -0.011728986640566869, 0.04731903765236058, 0.024278565233402327, -0.0202055220873499, -0.00344041726757616, -0.0059983107793590154, -0.04064541259318033, 0.016375954682992234, -0.01135368585284111, 0.03860618222275983, 0.025629004680896985, -0.008082578985205964, -0.04885045629851237, 0.028809704988296735, -0.02570286456190781, 0.006637146444963989, 0.035600884341260595, 0.025131795491315403, -0.05378137159965886, 0.04980907506456732, -0.04061273791969073, 0.02232726496054869, 0.0870364054535328, 0.11083698930537171, -0.009844311301486856, 0.021896946448464568, -0.013370645226224857, -0.04935768852153144, 0.055487858614111533, 0.04341156952912014, -0.04025193578871404], 'lda_topic_vector': [0.00037597768823616207, 0.00037597730988636613, 0.011616736650466919, 0.00037597701884806156, 0.00037597701884806156, 0.00037597701884806156, 0.9190030097961426, 0.06058431416749954, 0.00037597757182084024, 0.000375977746443823, 0.0003759779501706362, 0.0003759773971978575, 0.004660164937376976, 0.00037597757182084024, 0.00037597748450934887], 'keyword_coverage': 0.28, 'pronoun_usage': 0.010638297872340425, 'unique_word_proportion': 0.6914893617021277, 'long_word_proportion': 0.32978723404255317, 'imagery_word_proportion': 0.13829787234042554, 'positive_sentiment_score': 0.3301443001443001, 'negative_sentiment_score': 0.0, 'visual_word_proportion': 0.31382978723404253, 'unique_visual_word_proportion': 0.22340425531914893, 'average_imagery_score': 1.4047619047619047, 'discourse_marker_count': 0.09510869565217392, 'neural_coherence_score': 0.48606157302856445, 'longformer_sentence_embedding': [0.010260667651891708, 0.16882109642028809, -0.21883146464824677, -0.1291627436876297, -0.0067739104852080345, 0.006054803263396025, -0.09153050929307938, 0.022464685142040253, -0.01959330029785633, -0.010730051435530186, 0.2683364152908325, 0.06391146034002304, -0.1576027125120163, 0.1511964350938797, 0.06022452935576439, -0.1171572133898735, 0.08977240324020386, -0.02455991506576538, 0.26932790875434875, -0.17927931249141693, 0.013474845327436924, 0.139310821890831, 0.1889127790927887, 0.012442014180123806, -0.13470526039600372, -0.3147561848163605, -0.10720876604318619, -0.04819650202989578, 0.056917086243629456, 0.06033063679933548, -0.30468183755874634, -0.029371414333581924, 0.06522785127162933, -0.16922974586486816, 0.11066664755344391, -0.034019555896520615, -0.1532660275697708, -0.11314812302589417, -0.014977234415709972, -0.057627808302640915, 0.12745915353298187, 0.15302029252052307, -0.02507117949426174, -0.08531353622674942, 0.21142083406448364, 0.036467790603637695, -0.0006254192558117211, 0.03830844908952713, 0.0841178372502327, -0.06805834919214249, -0.0762878805398941, -0.5629207491874695, 0.12345463782548904, -0.3833685517311096, -0.1049659475684166, 0.06355395168066025, 0.11046134680509567, -0.017757629975676537, 0.07501663267612457, 0.05952886864542961, -0.15430517494678497, 0.05137605965137482, 0.1031811386346817, -0.010222058743238449, 0.1122002825140953, 0.11235947906970978, 0.0965844914317131, -0.1464885175228119, 0.16932015120983124, 0.16301508247852325, -0.45134660601615906, -0.024823172017931938, -0.18339066207408905, 0.11795470118522644, -0.00022711855126544833, -0.055410586297512054, -0.02591530978679657, 0.0922698974609375, 0.12481561303138733, 0.21122261881828308, -0.013401903212070465, -0.11989456415176392, -0.15662890672683716, 0.20577405393123627, -0.2663794755935669, 0.07905232906341553, 0.026673799380660057, -0.05752831697463989, 0.060809697955846786, 0.03127230703830719, -0.07923275232315063, 0.6220992803573608, 0.13477736711502075, -0.48233795166015625, 0.07771042734384537, -0.06760243326425552, -0.26717251539230347, 0.3053252398967743, -0.06210387125611305, -0.08046292513608932, -0.07607144117355347, -0.006656748242676258, 0.14711402356624603, 0.3173940181732178, -0.03072039596736431, -0.05261211842298508, -0.1659541130065918, -0.041315771639347076, -0.21174608170986176, 0.1552959829568863, 0.037548501044511795, -0.06853830814361572, 0.021087970584630966, -0.24110199511051178, -0.1935568004846573, -0.08170425146818161, 0.14606423676013947, 0.06430669873952866, 0.08016210049390793, 0.07022902369499207, 0.004352745600044727, -0.03637145087122917, 0.08483030647039413, -0.032217495143413544, 0.21813899278640747, -0.007137552369385958, -0.00794551707804203, -0.27282455563545227, 0.10508018732070923, -0.05335799604654312, -0.29627296328544617, -0.10558921098709106, 0.07651432603597641, 0.08506409078836441, -0.023019937798380852, -0.2740038335323334, 0.2040645331144333, 0.027353325858712196, -0.3431289792060852, 0.02313559502363205, -0.05823288485407829, -0.08986503630876541, -0.29330605268478394, 0.12290416657924652, -0.07076048851013184, -0.46183276176452637, -0.12633395195007324, -0.051756393164396286, 0.13474217057228088, -0.0655655786395073, -0.07317899167537689, 0.04621636122465134, 0.18790481984615326, -0.1373787373304367, 0.04551701620221138, 0.052648238837718964, 0.0971563383936882, 0.07287732511758804, 0.035120364278554916, -0.01834688149392605, 0.0782802477478981, -0.044561032205820084, -0.18707619607448578, 0.01901470310986042, -0.03126932680606842, 0.04274377226829529, -0.01591513119637966, 0.21838994324207306, -0.0735110193490982, 0.05169326812028885, -0.2742536664009094, 0.15936189889907837, 0.009895512834191322, 0.213689386844635, -0.08772645145654678, -0.11956661939620972, -0.03797179087996483, -0.022955745458602905, -0.011930818669497967, 0.15036626160144806, 0.01935095526278019, 0.02351146563887596, 0.010362767614424229, 0.17156006395816803, 0.20488297939300537, -0.1022079810500145, 0.016216279938817024, -0.09771543741226196, -0.23948386311531067, 0.03276102617383003, 0.24701614677906036, -0.16023945808410645, -0.03812326118350029, -0.04176416993141174, 0.10776501148939133, -0.04271741211414337, -0.0854683667421341, -0.03474421426653862, -0.12089954316616058, 0.055016156286001205, -0.09649280458688736, -0.2828304171562195, 0.2585260570049286, -0.20414148271083832, 0.3004623055458069, 0.2613234221935272, 0.05590561777353287, -0.09556647390127182, -0.03041507862508297, 0.05088420957326889, -0.020121807232499123, 0.18444474041461945, 0.09202149510383606, -0.01820983737707138, 0.0023034822661429644, -0.21258459985256195, 0.21125927567481995, 0.16718392074108124, 0.14769160747528076, -0.047420233488082886, 0.06693380326032639, 0.06500919908285141, 0.17797406017780304, 0.15618452429771423, -0.16119742393493652, -0.2932521402835846, -0.08230694383382797, -0.01817036047577858, 0.09796319156885147, -0.11990806460380554, -0.03077751211822033, -0.03692671284079552, 0.10963819175958633, 0.14349035918712616, 0.18796682357788086, -0.25846225023269653, -0.09596362709999084, -0.17517812550067902, 0.0838547796010971, 0.015545299276709557, -0.18862059712409973, -0.002108813961967826, 0.014672618359327316, 0.22232574224472046, -0.01209200918674469, -0.15730348229408264, -0.08975055068731308, 0.061205897480249405, 0.06612785905599594, 0.20482942461967468, 0.13065844774246216, 0.1078638955950737, 0.5071642398834229, -0.06808330863714218, 0.0942203626036644, 0.12035279721021652, 0.3008960783481598, -0.2507164776325226, -0.16740988194942474, 0.011112698353827, -0.17458224296569824, -0.0020040522795170546, 0.14108358323574066, -0.1279088705778122, 0.22287388145923615, 0.09501723200082779, 0.2926809787750244, -0.04670146480202675, 0.05367767810821533, -0.1511376053094864, -0.15649142861366272, -0.06671327352523804, -0.0355241484940052, 0.11253497004508972, 0.25294414162635803, 0.23737336695194244, 0.2181679755449295, -0.017251798883080482, 0.023653356358408928, 0.0006586442468687892, 0.8223810195922852, 0.051913704723119736, 0.11513516306877136, 0.08342766016721725, -0.11215769499540329, -0.26885324716567993, 0.13300445675849915, -0.0025747977197170258, -0.25877806544303894, 0.04064461961388588, -0.09827691316604614, -0.08732125163078308, 0.1434645652770996, 0.20910122990608215, 0.058788519352674484, -0.19930246472358704, 0.19034895300865173, -0.05715567618608475, 0.20302370190620422, -0.008669320493936539, -0.15372991561889648, -0.07020817697048187, -0.047805190086364746, -0.2533835172653198, 0.008785838261246681, -5.487411908688955e-05, 0.021194368600845337, -0.16598257422447205, -0.0405011847615242, -0.04542655125260353, 0.05004904046654701, -0.11940237879753113, -0.012267161160707474, 0.016694387421011925, -0.03274234011769295, -0.07443602383136749, 0.06374870985746384, -0.0422617606818676, -0.36957067251205444, -0.2512987554073334, -0.10677137970924377, -0.025804420933127403, -0.04347031190991402, -0.022652646526694298, -0.13373684883117676, -0.11407756060361862, -0.12952211499214172, 0.20279519259929657, -0.10040357708930969, 0.03342349827289581, -0.22935162484645844, -0.013726620934903622, -0.007941629737615585, 0.16517169773578644, -0.2529391348361969, 0.3193512558937073, 0.16681300103664398, -0.02040157839655876, -0.027976352721452713, -0.04144473746418953, 0.061260368674993515, -0.11454788595438004, 0.12475848197937012, -0.0016041129129007459, -0.1687035709619522, -0.08354787528514862, -0.16744756698608398, -0.12376821786165237, -0.1306765079498291, -0.07326426357030869, 0.12193695455789566, -0.14608871936798096, 0.13593940436840057, -0.13695988059043884, 0.20548726618289948, 0.16003313660621643, 0.23584143817424774, 0.1680999994277954, 0.04528772458434105, 0.08314734697341919, -0.13439695537090302, -0.004309542942792177, 0.25598037242889404, -0.17424623668193817, -0.08127406984567642, 0.010511954315006733, 0.10479804128408432, 0.20521973073482513, 0.05665621533989906, 0.19876541197299957, 0.1755368709564209, 0.16059796512126923, -0.13826322555541992, 0.13410499691963196, 0.027943970635533333, -0.07005047053098679, -0.017546143382787704, -0.10127290338277817, -0.1789172738790512, 0.01376769132912159, 0.13772538304328918, -0.017586447298526764, -0.1385832577943802, 0.07887126505374908, 0.13164667785167694, -0.2217596173286438, 0.11748327314853668, -0.08870429545640945, 0.3438872694969177, 0.01294618472456932, -0.02588278241455555, 0.09090214967727661, 0.08248906582593918, -0.13654391467571259, -0.08650582283735275, -0.014545467682182789, 0.06567617505788803, -0.1235627755522728, 0.1088862195611, 0.9336188435554504, -0.19343793392181396, -0.16647681593894958, 0.33034589886665344, -0.290813148021698, 0.019372792914509773, 0.09334146976470947, 0.10878981649875641, -0.11200283467769623, 0.08611942827701569, -0.15804453194141388, 0.01209633331745863, 0.2861807346343994, -0.13011996448040009, -0.13265912234783173, -0.12439871579408646, -0.2645677328109741, 0.04444298520684242, -0.012343235313892365, 0.13299600780010223, 0.07722540199756622, -0.05893872678279877, -0.25334441661834717, -0.05859442055225372, -0.051864489912986755, -0.3433629274368286, 0.030006587505340576, -0.03552776202559471, 0.032013893127441406, -0.11781466007232666, -0.1466389000415802, 0.07724659144878387, -0.012894261628389359, -0.20307496190071106, -0.10654960572719574, -0.07718279957771301, -0.0343707874417305, -0.20949113368988037, -0.304636687040329, 0.035749878734350204, -0.07287770509719849, 0.1516127586364746, 0.13369311392307281, 0.03647521510720253, -0.041935477405786514, -0.03763877972960472, -0.05894804745912552, 0.05393693968653679, -0.0017773737199604511, -0.16404704749584198, 0.04590784013271332, 0.007976570166647434, 0.032951194792985916, -0.1907966583967209, -0.11290570348501205, -0.08593028038740158, -0.07022018730640411, 0.19136464595794678, -0.0905834436416626, -0.08329564332962036, 0.2129708230495453, -0.3689511716365814, -0.21263356506824493, 0.04948273301124573, 0.1081419512629509, -0.06330371648073196, -0.1007925420999527, -0.3594668209552765, 0.1298283189535141, -0.09042029827833176, -0.02758435346186161, 0.5639581084251404, 0.13755880296230316, -0.16797399520874023, -0.09060477465391159, -0.08113183081150055, 0.0041146958246827126, -0.11378441751003265, 0.14907006919384003, 0.14383870363235474, -0.19829480350017548, 0.09127239882946014, -0.03537936508655548, 0.17836208641529083, 0.055036820471286774, -0.2782982289791107, 0.32729804515838623, 0.06866440176963806, -0.03782402351498604, -0.2058778554201126, 0.07348892837762833, 0.03241322562098503, 0.14497195184230804, -0.12768149375915527, -0.028822684660553932, 0.03689161688089371, -0.06437663733959198, -0.04021923243999481, -0.02428949438035488, 0.27153459191322327, 0.28342404961586, 0.1462751179933548, 0.0002665205392986536, 0.14340727031230927, -0.2655709385871887, -0.04260905832052231, 0.08770487457513809, -0.22060680389404297, 0.05165134370326996, -0.11218035966157913, -0.050115104764699936, -0.01783401146531105, 0.26708096265792847, -0.10258549451828003, -0.10276412218809128, -0.22428879141807556, 0.10855196416378021, -0.17904451489448547, 0.01853545941412449, 0.14974267780780792, 0.1847662627696991, -0.0892055481672287, 0.056584347039461136, -0.1334463506937027, -0.13759362697601318, 0.0783378854393959, 0.0016333645908161998, -0.060134682804346085, 0.04613995552062988, 0.036218781024217606, -0.015210294164717197, -0.05756688863039017, -0.13695091009140015, -0.12958906590938568, -0.008327837102115154, -0.12023072689771652, -0.21127641201019287, 0.012439441867172718, 0.06400956213474274, 0.026579339057207108, 0.6841341257095337, -0.23349443078041077, 0.12804406881332397, 0.0917486697435379, -0.2507249116897583, -0.07759235054254532, 0.07994842529296875, -0.17535845935344696, 2.4663140773773193, 0.2768471837043762, -0.1282585859298706, -0.1799924075603485, 0.14827610552310944, 0.010056964121758938, -0.14261309802532196, 0.03860849887132645, -0.0570247508585453, -0.03586630895733833, 0.28763020038604736, -0.08685767650604248, -0.07719553261995316, 0.15940889716148376, 0.23650556802749634, 0.12513454258441925, -0.07249124348163605, -0.10882647335529327, 0.10484711080789566, -0.09371049702167511, -0.004292344208806753, 0.25303593277931213, -0.08581388741731644, 0.2063896358013153, 0.05199887603521347, -0.30797111988067627, -0.27633780241012573, 0.19499722123146057, -0.05769418925046921, -0.07486383616924286, 0.05247543007135391, -0.024165159091353416, 0.01278380211442709, -0.033747509121894836, -0.0640806183218956, -0.07429346442222595, 0.05460992082953453, -0.08704753965139389, -0.006148656364530325, -0.17101836204528809, 0.021081654354929924, -0.05086095258593559, -0.10909821093082428, 0.10977842658758163, -0.23426435887813568, 0.19361667335033417, -0.14533917605876923, -0.04282636567950249, -0.08971720188856125, 0.06684800237417221, 0.07597657293081284, 0.1283709555864334, 0.07662363350391388, -0.008523192256689072, 0.16356922686100006, 0.05553753674030304, 0.09959486871957779, 0.009584134444594383, 0.1795807033777237, -0.1303420066833496, -0.01934514194726944, 0.2795911133289337, 0.007433835417032242, -0.39303579926490784, 0.05042767524719238, 0.07476329803466797, 0.13598507642745972, 0.008139844052493572, -0.0458354614675045, 0.509682834148407, -0.002422744408249855, 0.039697762578725815, -0.12037722766399384, 0.237367182970047, -0.2720668613910675, -0.25113382935523987, -0.1841195821762085, 0.06582484394311905, -0.14785952866077423, 0.16726940870285034, -0.0893755555152893, 0.2356775552034378, -0.22796577215194702, 0.06441246718168259, -28.232486724853516, 0.0066519007086753845, 0.028591753914952278, -0.21497006714344025, 0.15525422990322113, -0.12348737567663193, -0.19906723499298096, -0.06631497293710709, -0.18375927209854126, 0.007503541186451912, 0.542884349822998, -0.01346952561289072, -0.014847629703581333, 0.006109889596700668, -0.019416728988289833, 0.025546537712216377, -0.006396694574505091, -0.1272166520357132, 0.05202428996562958, -0.17705385386943817, 0.07277414947748184, -0.11562308669090271, 0.0211695097386837, 0.05224866792559624, 0.04348745197057724, 0.31167101860046387, 0.06377740949392319, 0.08468632400035858, 0.11529594659805298, -0.012087215669453144, -0.06466089934110641, -0.029560083523392677, 0.10562922805547714, -0.026919180527329445, 0.17424899339675903, 0.09418024122714996, -0.09500373154878616, 0.1195695698261261, -0.039807941764593124, 0.02086004801094532, -0.04250044375658035, 0.02143380604684353, -2.6831603050231934, -0.002450440777465701, 0.04253726825118065, -0.005567024927586317, -0.0014569914201274514, -0.20520706474781036, 0.014896240085363388, -0.04517202079296112, 0.16252894699573517, 0.15934601426124573, -0.01568078249692917, -0.03615973889827728, -3.4381002478767186e-05, 0.22676560282707214, 0.02820165455341339, 0.07736289501190186, -0.14297090470790863, 0.21418964862823486, 0.20901846885681152, -0.00031533034052699804, 0.09614523500204086, -0.21896325051784515, 0.012838167138397694, -0.1627490371465683, -0.1489911526441574, -0.15768152475357056, 0.008499251678586006, 0.04290952533483505, 0.04893866926431656, 0.017628077417612076, -0.09320651739835739, 0.15384045243263245, -0.012487228959798813, -0.006941957864910364, -0.09609805047512054, -0.03452880680561066, -0.08498268574476242, 0.004198398441076279, -0.13428638875484467, -0.06800708919763565, 0.08262807875871658, 0.20796583592891693, 0.12982702255249023, 0.11321090906858444, 0.09421706199645996, -0.054050784558057785, 0.146656334400177, -0.04545840993523598, 0.1845833957195282, -0.08525272458791733, 0.029502570629119873, 0.05997125059366226, -0.05802389979362488, -0.16508057713508606, -0.13383996486663818, -0.13879497349262238, 0.1184222474694252, 0.0028129012789577246, 0.036668021231889725, 0.024117818102240562, 0.02177785523235798, 0.05776185914874077, 0.046513881534338, 0.06284613162279129, 0.2743074595928192, 0.1430111676454544, 0.1247033178806305, -0.08877328038215637, 0.19564189016819, 0.2025013417005539, 0.2101145088672638, -0.20286354422569275, -0.12485615164041519, -0.01921244151890278, 0.12030057609081268, -0.15691830217838287, -0.1664305478334427, -0.06957080960273743, -0.11542084813117981, -0.29390111565589905, 0.191878080368042, 0.12186385691165924, 0.25385186076164246, 0.03548210486769676, -0.24598431587219238, -0.16268131136894226, 0.1942574679851532, 0.11054209619760513, 0.11360026895999908, -0.09416596591472626, 0.22915536165237427, -0.04346916452050209, 0.2263314127922058, 0.003154555568471551, -0.04726221039891243, -0.17836979031562805, -0.060578420758247375, -0.09491179883480072, -0.012911021709442139, 0.07856666296720505, 0.08460745960474014, -0.11781331151723862, 0.11018329858779907, -0.16949708759784698, -0.2581402659416199, -0.18774820864200592, -0.2004067301750183, -0.10166777670383453, 0.015889329835772514, -0.21227441728115082, 0.12312952429056168, 0.30731192231178284, -0.15483543276786804, -0.2995954155921936, 0.06553465127944946, -0.2267838567495346, -0.032973650842905045, 0.15889853239059448, 0.03321140259504318, 0.04766329005360603, 0.09486328810453415, 0.2579379379749298, -0.07226387411355972, 0.017777059227228165, -0.01465363148599863, -0.12000641971826553, 0.06524854898452759, 0.07035654038190842, 0.03866245225071907, -0.15461312234401703, -0.053987544029951096, 0.05998992919921875, 0.10280711203813553, 0.03225240483880043, 0.18385344743728638, 0.11123815923929214, 0.09310846775770187, -0.047823335975408554, 0.1054786741733551, -0.10868437588214874, 0.027525732293725014, -0.052922800183296204, 0.011374427936971188, -0.03904023393988609, -0.021542992442846298, -0.11826840043067932, 0.05422366410493851, -0.18155351281166077, -0.06678900867700577, -0.18593323230743408, 0.09973553568124771, 0.047262199223041534, 0.044428691267967224, 0.6066184043884277, -0.10982775688171387, -0.06267587840557098, 0.24398751556873322, -0.07522492855787277, -0.04973306506872177, 0.16805052757263184, -0.04546407610177994, -0.008023438975214958, 0.09656720608472824, -0.23839086294174194, -0.1208762601017952, 0.11961527168750763, 0.0984446257352829, 0.1257776916027069, -0.35781851410865784, 0.320469468832016, -0.05959388613700867, 0.025979844853281975, 0.014959138818085194, -0.10194606333971024, -0.09820623695850372, -0.13493402302265167, -0.10652265697717667, -0.14470990002155304, -0.022163577377796173, 0.006753250025212765, 0.02526100166141987, -0.15256726741790771, -0.09142608195543289, -0.05133838206529617, 0.08635801821947098, -0.15705761313438416, -0.15261338651180267, 0.11406267434358597, 0.04215129464864731, -0.1073634997010231, 0.07522553950548172, -0.08208384364843369, -0.021108832210302353, -0.07396036386489868, 0.05692166090011597, 0.0752323567867279, -0.18143188953399658, -0.031435612589120865, -0.15871372818946838, -0.18731951713562012, -0.26368606090545654, 0.07635198533535004, -0.002087509958073497, 0.1495896726846695, 0.0005973329534754157, -0.04109160229563713, -0.15682242810726166, -0.06326309591531754, 0.06448639929294586, -0.08549370616674423, -0.10300644487142563, 0.2370884120464325, -0.09181814640760422, 0.009364155121147633, -0.04714225232601166, -0.0707104280591011, -0.047534991055727005, -0.026498649269342422, 0.24683690071105957, 0.014786848798394203, 0.1341918557882309, 0.03120741993188858, 0.11649250239133835, -0.021418239921331406, 0.07118343561887741, -0.10776554048061371, 0.06423524022102356, 0.0486736036837101, -0.16551315784454346, -0.18839503824710846, 0.051693446934223175, 0.0012564458884298801, 0.18936100602149963, -0.021114246919751167, 0.013699641451239586, -0.03332342579960823, 0.06158028170466423, 0.12084215879440308, 0.017902212217450142, 0.03420983627438545, -0.025597400963306427, 0.9563150405883789, 0.0466313399374485, -0.007814156822860241, -0.12020917236804962, 0.11716967821121216, -0.15279120206832886, -0.13285502791404724, -0.19887840747833252, 0.08021210134029388, -0.01286983024328947, 0.03189748898148537, -0.0008485043072141707, 0.21130548417568207, 0.02131708711385727, 0.19320900738239288, -0.014085755683481693, -0.0008199546718969941, -0.017537560313940048, -0.00015168719983194023, -0.12365429848432541, -0.03907126560807228, 0.0233884509652853, -0.2593027353286743, 0.18899525701999664, -0.13246305286884308, 0.173058420419693, -0.38595810532569885, -0.16921159625053406, -0.17526446282863617, -0.08028191328048706, -0.013279936276376247, 0.018351128324866295, -0.06779544055461884, -0.09913279861211777, 0.1663038432598114, 0.1280367374420166, -0.11703798174858093, 0.032739702612161636, -0.2767750918865204, 0.07737113535404205, -0.1002732664346695, -0.28531011939048767, 0.061184871941804886, -0.15541011095046997, 0.036419063806533813, 0.13658297061920166, -0.1095634251832962, -0.041542522609233856, -0.15042658150196075, 0.1337975561618805, 0.06447510421276093, -0.07166528701782227, -0.3744516968727112, 0.008448264561593533, 0.07517343014478683, -0.09506021440029144, -0.04594746604561806, -0.07332299649715424, 0.039238687604665756, -0.32228103280067444, 0.024029945954680443, -0.15551437437534332, -0.16701510548591614, -0.04737086966633797, 0.011612586677074432, 0.237986221909523, 0.04214891046285629, -0.09996321052312851, -0.12207236140966415, -0.10468893498182297, 0.22824731469154358, -0.026490265503525734, -0.2995226979255676, -0.016882695257663727, -0.12228219956159592, 0.04623860493302345, 0.09085183590650558, -0.16225716471672058, -0.22379548847675323, 0.13956855237483978, 0.03528788313269615, 0.014492006972432137, 0.16914159059524536, 0.11929662525653839, -0.08804044127464294, -0.12134808301925659, -0.3378337621688843, 0.24374711513519287, -0.10789287090301514, 0.08722864091396332, -0.03205835446715355, -0.05175117775797844, 0.05890451371669769, -0.25189992785453796, 0.15282505750656128, 0.16745588183403015, -0.04489804431796074, 0.005501072853803635, -0.041075803339481354, 0.13084208965301514, -0.23985815048217773, 0.20621001720428467, -0.09158127754926682, -0.12226220220327377, 0.15662351250648499, -0.0667230561375618, 0.13415241241455078, -0.21382005512714386, -0.17844001948833466, 0.32537174224853516], 'longformer_coherence_score': 0.9929037094116211, 'type_token_ratio': 0.6968085106382979, 'lexical_diversity': 131.7461816162177, 'vocabulary_maturity': 263.03067484662574}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Vocabulary Maturity using frequency as a proxy\n",
    "from nltk.corpus import brown  # Brown corpus for word frequencies\n",
    "word_frequencies = Counter(brown.words())  # Get frequencies from a standard corpus\n",
    "\n",
    "def calculate_frequency_score(words):\n",
    "    \"\"\"Calculates average frequency of words in a standard corpus, with lower frequencies indicating more sophisticated vocabulary.\"\"\"\n",
    "    frequencies = [word_frequencies[word.lower()] for word in words if word.lower() in word_frequencies]\n",
    "    return sum(frequencies) / len(frequencies) if frequencies else 0\n",
    "\n",
    "def extract_vocabulary_features(example):\n",
    "    # Tokenize and filter words\n",
    "    words = [word for word in word_tokenize(example['full_text'].lower()) if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    # Type-Token Ratio (TTR) via spaCy\n",
    "    doc = nlp(\" \".join(words))\n",
    "    unique_words = set(token.text for token in doc)\n",
    "    example['type_token_ratio'] = len(unique_words) / len(words) if words else 0\n",
    "\n",
    "    # Lexical Diversity (spacy-based diversity metric)\n",
    "    example['lexical_diversity'] = doc._.mtld if hasattr(doc._, 'mtld') else ld.mtld(words)  # Ensure spaCy extension or fallback\n",
    "    \n",
    "    # Vocabulary Maturity (using word frequency as proxy for rarity/sophistication)\n",
    "    example['vocabulary_maturity'] = calculate_frequency_score(words)\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(extract_vocabulary_features)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dataset and push it to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Preview:\n",
      "Dataset({\n",
      "    features: ['essay_id', 'full_text', 'score', 'unique_mistakes', 'repeated_mistakes_count', 'max_repeated_mistake', 'word_count', 'flesch_reading_ease', 'flesch_kincaid_grade', 'sentence_count', 'average_sentence_length', 'pos_noun_count', 'pos_verb_count', 'pos_adj_count', 'pos_adv_count', 'grammar_error_count', 'syntactic_complexity', 'spelling_mistake_count', 'error_density', 'tfidf_keywords_vector', 'lda_topic_vector', 'keyword_coverage', 'pronoun_usage', 'unique_word_proportion', 'long_word_proportion', 'imagery_word_proportion', 'positive_sentiment_score', 'negative_sentiment_score', 'visual_word_proportion', 'unique_visual_word_proportion', 'average_imagery_score', 'discourse_marker_count', 'neural_coherence_score', 'longformer_sentence_embedding', 'longformer_coherence_score', 'type_token_ratio', 'lexical_diversity', 'vocabulary_maturity'],\n",
      "    num_rows: 13845\n",
      "})\n",
      "\n",
      "Eval Dataset Preview:\n",
      "Dataset({\n",
      "    features: ['essay_id', 'full_text', 'score', 'unique_mistakes', 'repeated_mistakes_count', 'max_repeated_mistake', 'word_count', 'flesch_reading_ease', 'flesch_kincaid_grade', 'sentence_count', 'average_sentence_length', 'pos_noun_count', 'pos_verb_count', 'pos_adj_count', 'pos_adv_count', 'grammar_error_count', 'syntactic_complexity', 'spelling_mistake_count', 'error_density', 'tfidf_keywords_vector', 'lda_topic_vector', 'keyword_coverage', 'pronoun_usage', 'unique_word_proportion', 'long_word_proportion', 'imagery_word_proportion', 'positive_sentiment_score', 'negative_sentiment_score', 'visual_word_proportion', 'unique_visual_word_proportion', 'average_imagery_score', 'discourse_marker_count', 'neural_coherence_score', 'longformer_sentence_embedding', 'longformer_coherence_score', 'type_token_ratio', 'lexical_diversity', 'vocabulary_maturity'],\n",
      "    num_rows: 3462\n",
      "})\n",
      "\n",
      "Test Dataset Preview:\n",
      "Dataset({\n",
      "    features: ['essay_id', 'full_text', 'score', 'unique_mistakes', 'repeated_mistakes_count', 'max_repeated_mistake', 'word_count', 'flesch_reading_ease', 'flesch_kincaid_grade', 'sentence_count', 'average_sentence_length', 'pos_noun_count', 'pos_verb_count', 'pos_adj_count', 'pos_adv_count', 'grammar_error_count', 'syntactic_complexity', 'spelling_mistake_count', 'error_density', 'tfidf_keywords_vector', 'lda_topic_vector', 'keyword_coverage', 'pronoun_usage', 'unique_word_proportion', 'long_word_proportion', 'imagery_word_proportion', 'positive_sentiment_score', 'negative_sentiment_score', 'visual_word_proportion', 'unique_visual_word_proportion', 'average_imagery_score', 'discourse_marker_count', 'neural_coherence_score', 'longformer_sentence_embedding', 'longformer_coherence_score', 'type_token_ratio', 'lexical_diversity', 'vocabulary_maturity'],\n",
      "    num_rows: 3\n",
      "})\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/leonkrug/.cache/huggingface/token\n",
      "Login successful\n",
      "0.0\n",
      "0.31452277302742004\n",
      "0.9922507405281067\n",
      "0.0\n",
      "0.3099924325942993\n",
      "0.9921725988388062\n",
      "0.0\n",
      "0.4460940659046173\n",
      "0.9929196834564209\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d95d64781a48b38622db0b35acc38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f0f50101304a0bb5a703748a7ed838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1636bfd70444283b711b32ee8abba6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f27a610d594b5292faf35e844216c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daa4978a6024109b6649551f3007c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7455bde52c114733a11faad362295ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47012ceed154d35a4851fa170a4f5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6833ee11f1a741efad60e992c0cf04d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All splits have been successfully pushed to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import os\n",
    "\n",
    "# Display the train, eval, and test splits\n",
    "print(\"Train Dataset Preview:\")\n",
    "print(dataset['train'])\n",
    "\n",
    "print(\"\\nEval Dataset Preview:\")\n",
    "print(dataset['eval'])\n",
    "\n",
    "print(\"\\nTest Dataset Preview:\")\n",
    "print(dataset['test'])\n",
    "\n",
    "# Login to Hugging Face Hub using your token (ensure HUGGINGFACE_TOKEN is set in environment)\n",
    "huggingface_hub.login(token=os.getenv('HUGGINGFACE_TOKEN'))\n",
    "\n",
    "# Define the repository name, user, and make sure it is set to private\n",
    "repo_id = f\"{huggingface_username}/{competition}_V2\"\n",
    "\n",
    "# Convert the test split to a DataFrame (or dictionary) to redefine it with the correct schema\n",
    "test_data = dataset['test'].to_pandas()\n",
    "\n",
    "# Redefine the data types for the fields causing issues\n",
    "test_data = test_data.astype({\n",
    "    'negative_sentiment_score': 'float64',\n",
    "    'neural_coherence_score': 'float64',\n",
    "    'longformer_coherence_score': 'float64'\n",
    "})\n",
    "\n",
    "# Re-convert the DataFrame back to a Hugging Face Dataset with the corrected schema\n",
    "dataset['test'] = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Create a new 'score' column with -1 in the 'test' split if 'score' does not already exist\n",
    "if 'score' not in dataset['test'].column_names:\n",
    "    dataset['test'] = dataset['test'].add_column('score', [-1] * len(dataset['test']))\n",
    "\n",
    "# Push each split to the Hub\n",
    "dataset['train'].push_to_hub(repo_id, private=True, split=\"train\")\n",
    "dataset['eval'].push_to_hub(repo_id, private=True, split=\"eval\")\n",
    "dataset['test'].push_to_hub(repo_id, private=True, split=\"test\")\n",
    "\n",
    "print(\"All splits have been successfully pushed to Hugging Face Hub.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
