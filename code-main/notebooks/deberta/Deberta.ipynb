{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version after installation: 1.26.4\n",
      "Logging in to Hugging Face Hub and W&B...\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\janni\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjannine-meier\u001b[0m (\u001b[33mnlp_janninemeier\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\janni\\_netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful.\n",
      "Loading dataset from Hugging Face...\n",
      "Dataset loaded successfully.\n",
      "Creating a smaller sample of 100 examples for training and evaluation...\n",
      "Sample size: 80 training examples and 20 evaluation examples.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import huggingface_hub\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import EarlyStoppingCallback, DebertaTokenizer, DebertaForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to uninstall and install packages\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"numpy\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install the compatible version of NumPy\n",
    "install(\"numpy<2.0\")\n",
    "\n",
    "\n",
    "# Print the installed NumPy version for verification\n",
    "print(\"NumPy version after installation:\", np.__version__)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "# Load dataset from Hugging Face hub\n",
    "huggingface_username = 'HSLU-AICOMP-LearningAgencyLab'\n",
    "competition = 'learning-agency-lab-automated-essay-scoring-2'\n",
    "\n",
    "wandb_project = 'HSLU-AICOMP-LearningAgencyLab'\n",
    "wandb_entity = 'jannine-meier'\n",
    "\n",
    "# Login to Hugging Face and W&B\n",
    "print(\"Logging in to Hugging Face Hub and W&B...\")\n",
    "huggingface_hub.login(token=os.getenv('HUGGINGFACE_TOKEN'))\n",
    "wandb.login(key=os.getenv('WANDB_API_TOKEN'))\n",
    "print(\"Login successful.\")\n",
    "\n",
    "# Set up W&B project\n",
    "os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "\n",
    "# Define the sample size for testing\n",
    "sample_size = 100  # Total number of examples in the sample\n",
    "train_sample_size = int(0.8 * sample_size)  # 80% for training\n",
    "eval_sample_size = sample_size - train_sample_size  # 20% for evaluation\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "print(\"Loading dataset from Hugging Face...\")\n",
    "dataset = load_dataset(f\"{huggingface_username}/{competition}\")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Create smaller samples for testing\n",
    "# Create smaller samples for testing\n",
    "print(f\"Creating a smaller sample of {sample_size} examples for training and evaluation...\")\n",
    "train_dataset_sample = dataset['train'].select(range(train_sample_size))\n",
    "eval_dataset_sample = dataset['train'].select(range(train_sample_size, train_sample_size + eval_sample_size))\n",
    "print(f\"Sample size: {len(train_dataset_sample)} training examples and {len(eval_dataset_sample)} evaluation examples.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the training dataset sample: ['essay_id', 'full_text', 'score', 'unique_mistakes', 'repeated_mistakes_count', 'max_repeated_mistake', 'word_count', 'flesch_reading_ease', 'flesch_kincaid_grade']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in the training dataset sample:\", train_dataset_sample.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DeBERTa tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer for DeBERTa\n",
    "print(\"Initializing DeBERTa tokenizer...\")\n",
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['full_text'], truncation=True, padding='max_length', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training dataset sample...\n",
      "Training dataset tokenized successfully.\n",
      "Tokenizing evaluation dataset sample...\n",
      "Evaluation dataset tokenized successfully.\n",
      "Converting labels to float using DataFrame transformation...\n",
      "Labels converted to float.\n",
      "Formatting datasets for PyTorch...\n",
      "Datasets formatted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the datasets using the 'full_text' column\n",
    "print(\"Tokenizing training dataset sample...\")\n",
    "train_dataset_sample = train_dataset_sample.map(tokenize_function, batched=True)\n",
    "print(\"Training dataset tokenized successfully.\")\n",
    "\n",
    "print(\"Tokenizing evaluation dataset sample...\")\n",
    "eval_dataset_sample = eval_dataset_sample.map(tokenize_function, batched=True)\n",
    "print(\"Evaluation dataset tokenized successfully.\")\n",
    "\n",
    "# Convert labels to float using a different method\n",
    "print(\"Converting labels to float using DataFrame transformation...\")\n",
    "train_dataset_sample = train_dataset_sample.to_pandas()\n",
    "train_dataset_sample['score'] = train_dataset_sample['score'].astype(float)\n",
    "train_dataset_sample = Dataset.from_pandas(train_dataset_sample)\n",
    "\n",
    "eval_dataset_sample = eval_dataset_sample.to_pandas()\n",
    "eval_dataset_sample['score'] = eval_dataset_sample['score'].astype(float)\n",
    "eval_dataset_sample = Dataset.from_pandas(eval_dataset_sample)\n",
    "print(\"Labels converted to float.\")\n",
    "\n",
    "# Rename the 'score' column to 'labels' for training\n",
    "train_dataset_sample = train_dataset_sample.rename_column(\"score\", \"labels\")\n",
    "eval_dataset_sample = eval_dataset_sample.rename_column(\"score\", \"labels\")\n",
    "\n",
    "# Format datasets for PyTorch\n",
    "print(\"Formatting datasets for PyTorch...\")\n",
    "train_dataset_sample.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "eval_dataset_sample.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "print(\"Datasets formatted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DeBERTa model for sequence classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define DeBERTa model for regression\n",
    "print(\"Loading DeBERTa model for sequence classification...\")\n",
    "model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=1)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MSE Loss\n",
    "class MSELoss(nn.Module):\n",
    "    def forward(self, logits, labels):\n",
    "        loss = nn.MSELoss()\n",
    "        return loss(logits.view(-1), labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training arguments...\n",
      "Training arguments set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janni\\anaconda3\\envs\\aicomp2\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments with logging to W&B\n",
    "print(\"Setting up training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='wandb',\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='eval_loss'\n",
    ")\n",
    "print(\"Training arguments set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric (MSE)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.flatten()\n",
    "    mse = ((predictions - labels) ** 2).mean()\n",
    "    return {\"mse\": mse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Trainer...\n",
      "Trainer initialized successfully.\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janni\\Documents\\Gitlab\\AICOMP\\code\\notebooks\\wandb\\run-20241022_015014-zwyete9u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_janninemeier/HSLU-AICOMP-LearningAgencyLab/runs/zwyete9u' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/nlp_janninemeier/HSLU-AICOMP-LearningAgencyLab' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_janninemeier/HSLU-AICOMP-LearningAgencyLab' target=\"_blank\">https://wandb.ai/nlp_janninemeier/HSLU-AICOMP-LearningAgencyLab</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_janninemeier/HSLU-AICOMP-LearningAgencyLab/runs/zwyete9u' target=\"_blank\">https://wandb.ai/nlp_janninemeier/HSLU-AICOMP-LearningAgencyLab/runs/zwyete9u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ffac20be4a403a81c962f124d55fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2195, 'grad_norm': 30.267826080322266, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e7501a749046f48ba422b0ba1e062b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0042355060577393, 'eval_mse': 1.0042356252670288, 'eval_runtime': 18.7595, 'eval_samples_per_second': 1.066, 'eval_steps_per_second': 0.16, 'epoch': 1.0}\n",
      "{'loss': 0.9815, 'grad_norm': 26.720014572143555, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bb910bf6ad43a6b81a2a6783748cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9802132844924927, 'eval_mse': 0.9802131652832031, 'eval_runtime': 18.2407, 'eval_samples_per_second': 1.096, 'eval_steps_per_second': 0.164, 'epoch': 2.0}\n",
      "{'loss': 0.8606, 'grad_norm': 7.429288864135742, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2476bba6e4ab466cb93883970f24e36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7756630182266235, 'eval_mse': 0.7756629586219788, 'eval_runtime': 17.8185, 'eval_samples_per_second': 1.122, 'eval_steps_per_second': 0.168, 'epoch': 3.0}\n",
      "{'loss': 0.569, 'grad_norm': 19.62665367126465, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb0469ef0e24446bd116ab8638bf604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6897901296615601, 'eval_mse': 0.6897901296615601, 'eval_runtime': 27.325, 'eval_samples_per_second': 0.732, 'eval_steps_per_second': 0.11, 'epoch': 4.0}\n",
      "{'loss': 0.464, 'grad_norm': 7.6206889152526855, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c06e2b5cdf34737978e6f65902d3f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6521454453468323, 'eval_mse': 0.6521453857421875, 'eval_runtime': 29.9953, 'eval_samples_per_second': 0.667, 'eval_steps_per_second': 0.1, 'epoch': 5.0}\n",
      "{'train_runtime': 1229.058, 'train_samples_per_second': 0.325, 'train_steps_per_second': 0.041, 'train_loss': 1.6189266967773437, 'epoch': 5.0}\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "print(\"Initializing Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_sample,\n",
    "    eval_dataset=eval_dataset_sample,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "print(\"Trainer initialized successfully.\")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed.\")\n",
    "\n",
    "\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"Evaluating the model...\")\n",
    "# evaluation_results = trainer.evaluate()\n",
    "# print(\"\\nEvaluation Results:\")\n",
    "# print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fb0027d4f4487bbbe27e5d4f6ce3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Get the model predictions and labels from the evaluation dataset\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m predictions, labels \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(eval_dataset_sample)\n\u001b[0;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a function to convert continuous scores into grades (1 to 6)\n",
    "def bin_to_grades(scores):\n",
    "    # Use np.digitize to categorize scores into grades 1 through 6\n",
    "    # Bins define the cutoff points for each grade, e.g., 0.5 to 1.5 is grade 1, etc.\n",
    "    bins = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]  # The edges for binning\n",
    "    grades = np.digitize(scores, bins, right=True)\n",
    "    return grades\n",
    "\n",
    "# Get the model predictions and labels from the evaluation dataset\n",
    "print(\"Evaluating the model...\")\n",
    "predictions, labels = trainer.predict(eval_dataset_sample)\n",
    "predictions = predictions.flatten()\n",
    "labels = labels.flatten()\n",
    "\n",
    "# Convert both predictions and labels into grade categories\n",
    "predicted_grades = bin_to_grades(predictions)\n",
    "true_grades = bin_to_grades(labels)\n",
    "\n",
    "# Calculate the accuracy of the predicted grades\n",
    "accuracy = accuracy_score(true_grades, predicted_grades)\n",
    "print(f\"Grade-based Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2910020108.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    predictions = trainer.predict(eval_dataset_sample`)\u001b[0m\n\u001b[1;37m                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "print(\"Generating predictions on the test set...\")\n",
    "predictions = trainer.predict(eval_dataset_sample`)\n",
    "predicted_scores = predictions.predictions.flatten()\n",
    "print(\"Predictions generated successfully.\")\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "# print(\"Creating submission DataFrame...\")\n",
    "# test_data_df = pd.DataFrame(test_dataset['id'])\n",
    "# test_data_df['predicted_score'] = predicted_scores\n",
    "# submission_path = 'submission.csv'\n",
    "# test_data_df[['id', 'predicted_score']].to_csv(submission_path, index=False)\n",
    "# print(f\"Submission file saved to {submission_path}.\")\n",
    "\n",
    "# Finalize W&B run\n",
    "print(\"Finalizing W&B run...\")\n",
    "wandb.finish()\n",
    "print(\"All done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
