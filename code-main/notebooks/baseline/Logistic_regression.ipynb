{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\janni\\_netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in to Hugging Face Hub and W&B...\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\janni\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Login successful.\n",
      "Initializing a W&B run...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janni\\Documents\\Gitlab_2\\code\\notebooks\\wandb\\run-20241025_130330-ynt3oasu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jannine-meier/HSLU-AICOMP-LearningAgencyLab/runs/ynt3oasu' target=\"_blank\">lyric-smoke-24</a></strong> to <a href='https://wandb.ai/jannine-meier/HSLU-AICOMP-LearningAgencyLab' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jannine-meier/HSLU-AICOMP-LearningAgencyLab' target=\"_blank\">https://wandb.ai/jannine-meier/HSLU-AICOMP-LearningAgencyLab</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jannine-meier/HSLU-AICOMP-LearningAgencyLab/runs/ynt3oasu' target=\"_blank\">https://wandb.ai/jannine-meier/HSLU-AICOMP-LearningAgencyLab/runs/ynt3oasu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run initialized.\n",
      "Loading the entire dataset from Hugging Face...\n",
      "Dataset loaded successfully.\n",
      "Dataset split into 13845 training examples and 3462 evaluation examples.\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import huggingface_hub\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "# Hugging Face and Weights & Biases setup\n",
    "huggingface_username = 'HSLU-AICOMP-LearningAgencyLab'\n",
    "competition = 'learning-agency-lab-automated-essay-scoring-2'\n",
    "\n",
    "wandb_project = 'HSLU-AICOMP-LearningAgencyLab'\n",
    "wandb_entity = 'jannine-meier'\n",
    "\n",
    "# Login to Hugging Face and W&B\n",
    "print(\"Logging in to Hugging Face Hub and W&B...\")\n",
    "huggingface_hub.login(token=os.getenv('HUGGINGFACE_TOKEN'))\n",
    "wandb.login(key=os.getenv('WANDB_API_TOKEN'))\n",
    "print(\"Login successful.\")\n",
    "\n",
    "# Initialize a W&B run\n",
    "print(\"Initializing a W&B run...\")\n",
    "wandb.init(project=wandb_project, entity=wandb_entity, config={\n",
    "    \"max_iter\": 1000, #set it to whatever above 1000 - converges early anywy\n",
    "    \"C\": 10, # 0.1 strong - 1 moderate - 10 weak (overfitting potential)\n",
    "    \"cv_folds\": 10 # the higher the more training data (10 = 90%, 5 = 20%)\n",
    "})\n",
    "print(\"W&B run initialized.\")\n",
    "\n",
    "# Load the entire dataset from Hugging Face\n",
    "print(\"Loading the entire dataset from Hugging Face...\")\n",
    "dataset = load_dataset(f\"{huggingface_username}/{competition}\")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_df = dataset['train'].to_pandas()\n",
    "eval_df = dataset['eval'].to_pandas()\n",
    "print(f\"Dataset split into {len(train_df)} training examples and {len(eval_df)} evaluation examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying TF-IDF vectorization to the text data...\n",
      "TF-IDF vectorization completed.\n",
      "Extracted target labels for training and evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data using TF-IDF\n",
    "print(\"Applying TF-IDF vectorization to the text data...\")\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train = vectorizer.fit_transform(train_df['full_text'])\n",
    "X_eval = vectorizer.transform(eval_df['full_text'])\n",
    "print(\"TF-IDF vectorization completed.\")\n",
    "\n",
    "# Target labels\n",
    "y_train = train_df['score']\n",
    "y_eval = eval_df['score']\n",
    "print(\"Extracted target labels for training and evaluation.\")\n",
    "\n",
    "# Define the model with regularization and max iterations\n",
    "model = LogisticRegression(max_iter=wandb.config.max_iter, C=wandb.config.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10-fold cross-validation...\n",
      "Starting fold 1...\n",
      "Fold 1, QWK: 0.6320197283977413\n",
      "Starting fold 2...\n",
      "Fold 2, QWK: 0.6220322023105613\n",
      "Starting fold 3...\n",
      "Fold 3, QWK: 0.6246417847977237\n",
      "Starting fold 4...\n",
      "Fold 4, QWK: 0.6384844553674451\n",
      "Starting fold 5...\n",
      "Fold 5, QWK: 0.6633189612014373\n",
      "Starting fold 6...\n",
      "Fold 6, QWK: 0.6208431588045114\n",
      "Starting fold 7...\n",
      "Fold 7, QWK: 0.6220610289190712\n",
      "Starting fold 8...\n",
      "Fold 8, QWK: 0.6427582464670303\n",
      "Starting fold 9...\n",
      "Fold 9, QWK: 0.6552256987039596\n",
      "Starting fold 10...\n",
      "Fold 10, QWK: 0.6640211862625585\n"
     ]
    }
   ],
   "source": [
    "# Set up cross-validation\n",
    "cv_folds = wandb.config.cv_folds\n",
    "skf = StratifiedKFold(n_splits=cv_folds)\n",
    "print(f\"Starting {cv_folds}-fold cross-validation...\")\n",
    "\n",
    "# Perform cross-validation and log metrics\n",
    "cv_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"Starting fold {fold + 1}...\")\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Train the model on the current fold\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Evaluate on the validation fold\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    qwk = cohen_kappa_score(y_val_fold, y_val_pred, weights='quadratic')\n",
    "    \n",
    "    # Log metrics for the current fold\n",
    "    print(f\"Fold {fold + 1}, QWK: {qwk}\")\n",
    "    wandb.log({f\"fold_{fold+1}_qwk\": qwk})\n",
    "    \n",
    "    # Store the score for averaging later\n",
    "    cv_scores.append(qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average QWK across 10 folds: 0.6385406451232039\n",
      "Evaluating on the test set with the entire training data...\n",
      "Evaluation - QWK: 0.6522857466435952\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a911f8bc8f4b1a9c59d39d1f3e864b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_qwk</td><td>▁</td></tr><tr><td>eval_qwk</td><td>▁</td></tr><tr><td>fold_10_qwk</td><td>▁</td></tr><tr><td>fold_1_qwk</td><td>▁</td></tr><tr><td>fold_2_qwk</td><td>▁</td></tr><tr><td>fold_3_qwk</td><td>▁</td></tr><tr><td>fold_4_qwk</td><td>▁</td></tr><tr><td>fold_5_qwk</td><td>▁</td></tr><tr><td>fold_6_qwk</td><td>▁</td></tr><tr><td>fold_7_qwk</td><td>▁</td></tr><tr><td>fold_8_qwk</td><td>▁</td></tr><tr><td>fold_9_qwk</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_qwk</td><td>0.63854</td></tr><tr><td>eval_qwk</td><td>0.65229</td></tr><tr><td>fold_10_qwk</td><td>0.66402</td></tr><tr><td>fold_1_qwk</td><td>0.63202</td></tr><tr><td>fold_2_qwk</td><td>0.62203</td></tr><tr><td>fold_3_qwk</td><td>0.62464</td></tr><tr><td>fold_4_qwk</td><td>0.63848</td></tr><tr><td>fold_5_qwk</td><td>0.66332</td></tr><tr><td>fold_6_qwk</td><td>0.62084</td></tr><tr><td>fold_7_qwk</td><td>0.62206</td></tr><tr><td>fold_8_qwk</td><td>0.64276</td></tr><tr><td>fold_9_qwk</td><td>0.65523</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-smoke-24</strong> at: <a href='https://wandb.ai/jannine-meier/HSLU-AICOMP-LearningAgencyLab/runs/ynt3oasu' target=\"_blank\">https://wandb.ai/jannine-meier/HSLU-AICOMP-LearningAgencyLab/runs/ynt3oasu</a><br/> View project at: <a href='https://wandb.ai/jannine-meier/HSLU-AICOMP-LearningAgencyLab' target=\"_blank\">https://wandb.ai/jannine-meier/HSLU-AICOMP-LearningAgencyLab</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_130330-ynt3oasu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run finished.\n"
     ]
    }
   ],
   "source": [
    "# Calculate and log the average metrics across all folds\n",
    "avg_qwk = np.mean(cv_scores)\n",
    "print(f\"Average QWK across {cv_folds} folds: {avg_qwk}\")\n",
    "wandb.log({\"avg_qwk\": avg_qwk})\n",
    "\n",
    "# Evaluate the model on the evaluation set\n",
    "print(\"Evaluating on the test set with the entire training data...\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_eval = model.predict(X_eval)\n",
    "qwk_eval = cohen_kappa_score(y_eval, y_pred_eval, weights='quadratic')\n",
    "print(f\"Evaluation - QWK: {qwk_eval}\")\n",
    "\n",
    "# Log final evaluation results to W&B\n",
    "wandb.log({\"eval_qwk\": qwk_eval})\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n",
    "print(\"W&B run finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
