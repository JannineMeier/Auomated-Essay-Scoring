{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: HSLU-AICOMP-LearningAgencyLab/automated-essay-scoring-setfit (CPU-only)\n",
      "Loaded dataset: HSLU-AICOMP-LearningAgencyLab/featuers_labels_combined\n",
      "Dataset shape: (13845, 39)\n",
      "Filtered dataset shape (excluding 'In_Persuade_Corpus'): (3485, 39)\n",
      "Training dataset shape: (30, 39)\n",
      "Evaluation dataset shape: (3477, 39)\n",
      "Adjusted scores in training dataset: [0 1 2 3 4 5]\n",
      "Adjusted scores in evaluation dataset: [3 1 2 4 0 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janni\\AppData\\Local\\Temp\\ipykernel_24896\\933359513.py:33: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = filtered_df.groupby(\"score\").apply(lambda x: x.sample(5, random_state=42))\n",
      "C:\\Users\\janni\\AppData\\Local\\Temp\\ipykernel_24896\\933359513.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_df[\"score\"] = eval_df[\"score\"] - 1\n",
      "C:\\Users\\janni\\AppData\\Local\\Temp\\ipykernel_24896\\933359513.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_df[\"sentence1\"] = eval_df[\"full_text\"]\n",
      "C:\\Users\\janni\\AppData\\Local\\Temp\\ipykernel_24896\\933359513.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_df[\"sentence2\"] = eval_df[\"full_text\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels (scores): 6\n",
      "Starting CPU-optimized training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdbd7f820734b7da2f1dabfce55adff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import Dataset, load_dataset\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\n",
    "from sentence_transformers.losses import SoftmaxLoss\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sentence_transformers.evaluation import SequentialEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "# Set environment variables to optimize CPU usage\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"  # Set to the number of CPU cores available\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "\n",
    "# Define the dataset and model\n",
    "dataset_name = \"HSLU-AICOMP-LearningAgencyLab/featuers_labels_combined\"\n",
    "base_model_username = \"HSLU-AICOMP-LearningAgencyLab\"\n",
    "base_model_name = \"automated-essay-scoring-setfit\"\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(f\"{base_model_username}/{base_model_name}\")\n",
    "print(f\"Loaded model: {base_model_username}/{base_model_name} (CPU-only)\")\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "print(f\"Loaded dataset: {dataset_name}\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Filter out rows where 'In_Persuade_Corpus' is True\n",
    "filtered_df = df[df[\"In_Persuade_Corpus\"] == False]\n",
    "print(f\"Filtered dataset shape (excluding 'In_Persuade_Corpus'): {filtered_df.shape}\")\n",
    "\n",
    "# Group by 'score' and sample 5 instances per score for training (smaller sample size)\n",
    "train_df = filtered_df.groupby(\"score\").apply(lambda x: x.sample(5, random_state=42))\n",
    "train_df = train_df.droplevel(0).reset_index(drop=True)\n",
    "print(f\"Training dataset shape: {train_df.shape}\")\n",
    "\n",
    "# Create a mask for rows in the training set\n",
    "train_mask = filtered_df.index.isin(train_df.index)\n",
    "\n",
    "# Use the remaining rows as the evaluation set\n",
    "eval_df = filtered_df[~train_mask]\n",
    "print(f\"Evaluation dataset shape: {eval_df.shape}\")\n",
    "\n",
    "# Adjust scores to start from 0\n",
    "train_df[\"score\"] = train_df[\"score\"] - 1\n",
    "eval_df[\"score\"] = eval_df[\"score\"] - 1\n",
    "print(\"Adjusted scores in training dataset:\", train_df[\"score\"].unique())\n",
    "print(\"Adjusted scores in evaluation dataset:\", eval_df[\"score\"].unique())\n",
    "\n",
    "# Prepare the dataset for SoftmaxLoss\n",
    "# Duplicate `full_text` as sentence1 and sentence2 for compatibility with SoftmaxLoss\n",
    "train_df[\"sentence1\"] = train_df[\"full_text\"]\n",
    "train_df[\"sentence2\"] = train_df[\"full_text\"]\n",
    "\n",
    "eval_df[\"sentence1\"] = eval_df[\"full_text\"]\n",
    "eval_df[\"sentence2\"] = eval_df[\"full_text\"]\n",
    "\n",
    "# Convert to Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"sentence1\", \"sentence2\", \"score\"]])\n",
    "eval_dataset = Dataset.from_pandas(eval_df[[\"sentence1\", \"sentence2\", \"score\"]])\n",
    "\n",
    "# Prepare the loss function\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "num_labels = train_df[\"score\"].nunique()\n",
    "print(f\"Number of labels (scores): {num_labels}\")\n",
    "\n",
    "softmax_loss = SoftmaxLoss(\n",
    "    model, sentence_embedding_dimension=embedding_dim, num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Define evaluation metrics\n",
    "evaluators = []\n",
    "evaluator = SequentialEvaluator(evaluators, main_score_function=lambda scores: scores[-1] if scores else 0)\n",
    "\n",
    "# Define training arguments\n",
    "from sentence_transformers import SentenceTransformerTrainingArguments\n",
    "\n",
    "# Reduce batch size and number of epochs for CPU efficiency\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=f\"./{base_model_name}-fine-tuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"steps\",  # Updated deprecated argument\n",
    "    eval_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_sequential_score\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,  # Fewer epochs\n",
    "    warmup_steps=100,  # Adjusted warmup steps\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,\n",
    "    early_stopping_threshold=0.01,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=softmax_loss,\n",
    "    args=training_args,\n",
    "    evaluator=evaluator,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"Starting CPU-optimized training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "eval_results = trainer.evaluate(eval_dataset)\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Save the model locally\n",
    "#trainer.save_model(\"./fine_tuned_model\")\n",
    "print(\"Model saved locally!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique scores in training dataset: [1 2 3 4 5 6]\n",
      "Unique scores in evaluation dataset: [4 2 3 5 1 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique scores in training dataset:\", train_df[\"score\"].unique())\n",
    "print(\"Unique scores in evaluation dataset:\", eval_df[\"score\"].unique())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
